{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alecjeffery/Documents/Playgrounds/Python/tft_v1/env/lib/python3.11/site-packages/pytorch_forecasting/models/base_model.py:27: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alecjeffery/Documents/Playgrounds/Python/tft_v1/env/lib/python3.11/site-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 16 worker processes in total. Our suggested max number of worker in current system is 8 (`cpuset` is not taken into account), which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(\n",
      "/Users/alecjeffery/Documents/Playgrounds/Python/tft_v1/env/lib/python3.11/site-packages/lightning/pytorch/utilities/parsing.py:209: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "/Users/alecjeffery/Documents/Playgrounds/Python/tft_v1/env/lib/python3.11/site-packages/lightning/pytorch/utilities/parsing.py:209: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of params in network: 32358.6k\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alecjeffery/Documents/Playgrounds/Python/tft_v1/env/lib/python3.11/site-packages/pytorch_lightning/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TFTLightningModule(\n",
       "  (tft_model): TemporalFusionTransformer(\n",
       "    \t\"attention_head_size\":               4\n",
       "    \t\"categorical_groups\":                {}\n",
       "    \t\"causal_attention\":                  True\n",
       "    \t\"dataset_parameters\":                {'time_idx': 'time_idx', 'target': ['target_1', 'target_2'], 'group_ids': ['group'], 'weight': None, 'max_encoder_length': 90, 'min_encoder_length': 90, 'min_prediction_idx': 0, 'min_prediction_length': 5, 'max_prediction_length': 5, 'static_categoricals': ['group'], 'static_reals': None, 'time_varying_known_categoricals': None, 'time_varying_known_reals': ['time_idx'], 'time_varying_unknown_categoricals': None, 'time_varying_unknown_reals': ['open', 'high', 'low', 'target_1', 'target_2', 'vopen', 'vhigh', 'vlow', 'VIX', 'SPY', 'TNX', 'rsi14', 'rsi9', 'rsi24', 'MACD5355macddiff', 'MACD5355macddiffslope', 'MACD5355macd', 'MACD5355macdslope', 'MACD5355macdsig', 'MACD5355macdsigslope', 'MACD12269macddiff', 'MACD12269macddiffslope', 'MACD12269macd', 'MACD12269macdslope', 'MACD12269macdsig', 'MACD12269macdsigslope', 'lowTail', 'highTail', 'openTail', 'IntradayBar', 'IntradayRange', 'CloseOverSMA5', 'CloseOverSMA10', 'CloseOverSMA12', 'CloseOverSMA20', 'CloseOverSMA30', 'CloseOverSMA65', 'CloseOverSMA50', 'CloseOverSMA100', 'CloseOverSMA200', 'VolOverSMA5', 'VolOverSMA10', 'VolOverSMA12', 'VolOverSMA20', 'VolOverSMA30', 'VolOverSMA65', 'VolOverSMA50', 'VolOverSMA100', 'VolOverSMA200', 'Ret1day', 'Ret4day', 'Ret8day', 'Ret12day', 'Ret24day', 'Ret72day', 'Ret240day', 'RSC', 'bands_l', 'bands_u', 'ADX', 'cloudA', 'cloudB', 'closeVsIchA', 'closeVsIchB', 'IchAvIchB', 'CondVol_1', 'CondVol_4', 'CondVol_8', 'CondVol_12', 'CondVol_24', 'CondVol_72', 'CondVol_240', 'CV1vCV4', 'CV4vCV8', 'CV8vCV12', 'CV12vCV24', 'CV8vCV24', 'CV24vCV240', 'RSC_VIX', 'RSC_VIX_IV', 'RSC_VIX_real', 'RSC_VIX_IV_real', 'RSC_IV_gar', 'close_spy_corr22', 'close_tnx_corr22', 'vclose_VIX_corr22', 'garch_IV_corr22', 'close_spy_corr65', 'close_tnx_corr65', 'vclose_VIX_corr65', 'garch_IV_corr65', 'close_spy_corr252', 'close_tnx_corr252', 'vclose_VIX_corr252', 'garch_IV_corr252'], 'variable_groups': None, 'constant_fill_strategy': None, 'allow_missing_timesteps': False, 'lags': None, 'add_relative_time_idx': False, 'add_target_scales': False, 'add_encoder_length': False, 'target_normalizer': MultiNormalizer(\n",
       "    \t\tnormalizers=[TorchNormalizer(method='identity', center=True, transformation=None, method_kwargs=None), TorchNormalizer(method='identity', center=True, transformation=None, method_kwargs=None)]\n",
       "    \t), 'categorical_encoders': {'__group_id__group': NaNLabelEncoder(add_nan=False, warn=True), 'group': NaNLabelEncoder(add_nan=False, warn=True)}, 'scalers': {'time_idx': StandardScaler(), 'open': StandardScaler(), 'high': StandardScaler(), 'low': StandardScaler(), 'vopen': StandardScaler(), 'vhigh': StandardScaler(), 'vlow': StandardScaler(), 'VIX': StandardScaler(), 'SPY': StandardScaler(), 'TNX': StandardScaler(), 'rsi14': StandardScaler(), 'rsi9': StandardScaler(), 'rsi24': StandardScaler(), 'MACD5355macddiff': StandardScaler(), 'MACD5355macddiffslope': StandardScaler(), 'MACD5355macd': StandardScaler(), 'MACD5355macdslope': StandardScaler(), 'MACD5355macdsig': StandardScaler(), 'MACD5355macdsigslope': StandardScaler(), 'MACD12269macddiff': StandardScaler(), 'MACD12269macddiffslope': StandardScaler(), 'MACD12269macd': StandardScaler(), 'MACD12269macdslope': StandardScaler(), 'MACD12269macdsig': StandardScaler(), 'MACD12269macdsigslope': StandardScaler(), 'lowTail': StandardScaler(), 'highTail': StandardScaler(), 'openTail': StandardScaler(), 'IntradayBar': StandardScaler(), 'IntradayRange': StandardScaler(), 'CloseOverSMA5': StandardScaler(), 'CloseOverSMA10': StandardScaler(), 'CloseOverSMA12': StandardScaler(), 'CloseOverSMA20': StandardScaler(), 'CloseOverSMA30': StandardScaler(), 'CloseOverSMA65': StandardScaler(), 'CloseOverSMA50': StandardScaler(), 'CloseOverSMA100': StandardScaler(), 'CloseOverSMA200': StandardScaler(), 'VolOverSMA5': StandardScaler(), 'VolOverSMA10': StandardScaler(), 'VolOverSMA12': StandardScaler(), 'VolOverSMA20': StandardScaler(), 'VolOverSMA30': StandardScaler(), 'VolOverSMA65': StandardScaler(), 'VolOverSMA50': StandardScaler(), 'VolOverSMA100': StandardScaler(), 'VolOverSMA200': StandardScaler(), 'Ret1day': StandardScaler(), 'Ret4day': StandardScaler(), 'Ret8day': StandardScaler(), 'Ret12day': StandardScaler(), 'Ret24day': StandardScaler(), 'Ret72day': StandardScaler(), 'Ret240day': StandardScaler(), 'RSC': StandardScaler(), 'bands_l': StandardScaler(), 'bands_u': StandardScaler(), 'ADX': StandardScaler(), 'cloudA': StandardScaler(), 'cloudB': StandardScaler(), 'closeVsIchA': StandardScaler(), 'closeVsIchB': StandardScaler(), 'IchAvIchB': StandardScaler(), 'CondVol_1': StandardScaler(), 'CondVol_4': StandardScaler(), 'CondVol_8': StandardScaler(), 'CondVol_12': StandardScaler(), 'CondVol_24': StandardScaler(), 'CondVol_72': StandardScaler(), 'CondVol_240': StandardScaler(), 'CV1vCV4': StandardScaler(), 'CV4vCV8': StandardScaler(), 'CV8vCV12': StandardScaler(), 'CV12vCV24': StandardScaler(), 'CV8vCV24': StandardScaler(), 'CV24vCV240': StandardScaler(), 'RSC_VIX': StandardScaler(), 'RSC_VIX_IV': StandardScaler(), 'RSC_VIX_real': StandardScaler(), 'RSC_VIX_IV_real': StandardScaler(), 'RSC_IV_gar': StandardScaler(), 'close_spy_corr22': StandardScaler(), 'close_tnx_corr22': StandardScaler(), 'vclose_VIX_corr22': StandardScaler(), 'garch_IV_corr22': StandardScaler(), 'close_spy_corr65': StandardScaler(), 'close_tnx_corr65': StandardScaler(), 'vclose_VIX_corr65': StandardScaler(), 'garch_IV_corr65': StandardScaler(), 'close_spy_corr252': StandardScaler(), 'close_tnx_corr252': StandardScaler(), 'vclose_VIX_corr252': StandardScaler(), 'garch_IV_corr252': StandardScaler()}, 'randomize_length': None, 'predict_mode': False}\n",
       "    \t\"dropout\":                           0.35\n",
       "    \t\"embedding_labels\":                  {'group': {'AAPL': 0, 'ABBV': 1, 'ABT': 2, 'ACN': 3, 'ADBE': 4, 'ADI': 5, 'ADP': 6, 'ADSK': 7, 'AEP': 8, 'AFL': 9, 'AIG': 10, 'AJG': 11, 'ALL': 12, 'AMAT': 13, 'AMGN': 14, 'AMP': 15, 'AMT': 16, 'AMZN': 17, 'AON': 18, 'APD': 19, 'APH': 20, 'AXP': 21, 'AZO': 22, 'BA': 23, 'BAC': 24, 'BDX': 25, 'BK': 26, 'BKNG': 27, 'BLK': 28, 'BMY': 29, 'BRK B': 30, 'BSX': 31, 'BX': 32, 'C': 33, 'CAT': 34, 'CB': 35, 'CDNS': 36, 'CI': 37, 'CL': 38, 'CMCSA': 39, 'CME': 40, 'CMG': 41, 'CMI': 42, 'COF': 43, 'COP': 44, 'COST': 45, 'CPRT': 46, 'CRM': 47, 'CSCO': 48, 'CTAS': 49, 'CVS': 50, 'CVX': 51, 'D': 52, 'DE': 53, 'DFS': 54, 'DHR': 55, 'DIS': 56, 'DLR': 57, 'DUK': 58, 'ECL': 59, 'ELV': 60, 'EMR': 61, 'EOG': 62, 'EQIX': 63, 'ETN': 64, 'FCX': 65, 'FDX': 66, 'FI': 67, 'FTNT': 68, 'GD': 69, 'GE': 70, 'GILD': 71, 'GM': 72, 'GOOGL': 73, 'GS': 74, 'HCA': 75, 'HD': 76, 'HON': 77, 'HWM': 78, 'IBM': 79, 'ICE': 80, 'INTC': 81, 'INTU': 82, 'ISRG': 83, 'ITW': 84, 'JCI': 85, 'JNJ': 86, 'JPM': 87, 'KKR': 88, 'KLAC': 89, 'KMB': 90, 'KMI': 91, 'KO': 92, 'LLY': 93, 'LMT': 94, 'LOW': 95, 'LRCX': 96, 'MA': 97, 'MCD': 98, 'MCK': 99, 'MCO': 100, 'MDLZ': 101, 'MDT': 102, 'MET': 103, 'META': 104, 'MMC': 105, 'MMM': 106, 'MO': 107, 'MPC': 108, 'MRK': 109, 'MS': 110, 'MSFT': 111, 'MSI': 112, 'MU': 113, 'NEE': 114, 'NEM': 115, 'NFLX': 116, 'NKE': 117, 'NOC': 118, 'NOW': 119, 'NSC': 120, 'NVDA': 121, 'NXPI': 122, 'O': 123, 'OKE': 124, 'ORCL': 125, 'ORLY': 126, 'PANW': 127, 'PAYX': 128, 'PCAR': 129, 'PFE': 130, 'PG': 131, 'PGR': 132, 'PH': 133, 'PLD': 134, 'PM': 135, 'PNC': 136, 'PSA': 137, 'PSX': 138, 'QCOM': 139, 'RCL': 140, 'REGN': 141, 'ROP': 142, 'RSG': 143, 'RTX': 144, 'SBUX': 145, 'SCHW': 146, 'SHW': 147, 'SLB': 148, 'SNPS': 149, 'SO': 150, 'SPG': 151, 'SPGI': 152, 'SRE': 153, 'SYK': 154, 'T': 155, 'TDG': 156, 'TFC': 157, 'TGT': 158, 'TJX': 159, 'TMO': 160, 'TRV': 161, 'TSLA': 162, 'TT': 163, 'TXN': 164, 'UNH': 165, 'UNP': 166, 'UPS': 167, 'USB': 168, 'V': 169, 'VRTX': 170, 'VZ': 171, 'WELL': 172, 'WFC': 173, 'WM': 174, 'WMB': 175, 'WMT': 176, 'XOM': 177, 'ZTS': 178}}\n",
       "    \t\"embedding_paddings\":                []\n",
       "    \t\"embedding_sizes\":                   {'group': (179, 29)}\n",
       "    \t\"hidden_continuous_size\":            256\n",
       "    \t\"hidden_continuous_sizes\":           {}\n",
       "    \t\"hidden_size\":                       256\n",
       "    \t\"learning_rate\":                     4.171863120490385e-05\n",
       "    \t\"log_gradient_flow\":                 False\n",
       "    \t\"log_interval\":                      200\n",
       "    \t\"log_val_interval\":                  200\n",
       "    \t\"lstm_layers\":                       2\n",
       "    \t\"max_encoder_length\":                90\n",
       "    \t\"monotone_constaints\":               {}\n",
       "    \t\"monotone_constraints\":              {}\n",
       "    \t\"optimizer\":                         adam\n",
       "    \t\"optimizer_params\":                  None\n",
       "    \t\"output_size\":                       [1, 1]\n",
       "    \t\"output_transformer\":                MultiNormalizer(\n",
       "    \t\tnormalizers=[TorchNormalizer(method='identity', center=True, transformation=None, method_kwargs=None), TorchNormalizer(method='identity', center=True, transformation=None, method_kwargs=None)]\n",
       "    \t)\n",
       "    \t\"reduce_on_plateau_min_lr\":          1e-05\n",
       "    \t\"reduce_on_plateau_patience\":        5\n",
       "    \t\"reduce_on_plateau_reduction\":       2.0\n",
       "    \t\"share_single_variable_networks\":    False\n",
       "    \t\"static_categoricals\":               ['group']\n",
       "    \t\"static_reals\":                      []\n",
       "    \t\"time_varying_categoricals_decoder\": []\n",
       "    \t\"time_varying_categoricals_encoder\": []\n",
       "    \t\"time_varying_reals_decoder\":        ['time_idx']\n",
       "    \t\"time_varying_reals_encoder\":        ['time_idx', 'open', 'high', 'low', 'target_1', 'target_2', 'vopen', 'vhigh', 'vlow', 'VIX', 'SPY', 'TNX', 'rsi14', 'rsi9', 'rsi24', 'MACD5355macddiff', 'MACD5355macddiffslope', 'MACD5355macd', 'MACD5355macdslope', 'MACD5355macdsig', 'MACD5355macdsigslope', 'MACD12269macddiff', 'MACD12269macddiffslope', 'MACD12269macd', 'MACD12269macdslope', 'MACD12269macdsig', 'MACD12269macdsigslope', 'lowTail', 'highTail', 'openTail', 'IntradayBar', 'IntradayRange', 'CloseOverSMA5', 'CloseOverSMA10', 'CloseOverSMA12', 'CloseOverSMA20', 'CloseOverSMA30', 'CloseOverSMA65', 'CloseOverSMA50', 'CloseOverSMA100', 'CloseOverSMA200', 'VolOverSMA5', 'VolOverSMA10', 'VolOverSMA12', 'VolOverSMA20', 'VolOverSMA30', 'VolOverSMA65', 'VolOverSMA50', 'VolOverSMA100', 'VolOverSMA200', 'Ret1day', 'Ret4day', 'Ret8day', 'Ret12day', 'Ret24day', 'Ret72day', 'Ret240day', 'RSC', 'bands_l', 'bands_u', 'ADX', 'cloudA', 'cloudB', 'closeVsIchA', 'closeVsIchB', 'IchAvIchB', 'CondVol_1', 'CondVol_4', 'CondVol_8', 'CondVol_12', 'CondVol_24', 'CondVol_72', 'CondVol_240', 'CV1vCV4', 'CV4vCV8', 'CV8vCV12', 'CV12vCV24', 'CV8vCV24', 'CV24vCV240', 'RSC_VIX', 'RSC_VIX_IV', 'RSC_VIX_real', 'RSC_VIX_IV_real', 'RSC_IV_gar', 'close_spy_corr22', 'close_tnx_corr22', 'vclose_VIX_corr22', 'garch_IV_corr22', 'close_spy_corr65', 'close_tnx_corr65', 'vclose_VIX_corr65', 'garch_IV_corr65', 'close_spy_corr252', 'close_tnx_corr252', 'vclose_VIX_corr252', 'garch_IV_corr252']\n",
       "    \t\"weight_decay\":                      0.0\n",
       "    \t\"x_categoricals\":                    ['group']\n",
       "    \t\"x_reals\":                           ['time_idx', 'open', 'high', 'low', 'target_1', 'target_2', 'vopen', 'vhigh', 'vlow', 'VIX', 'SPY', 'TNX', 'rsi14', 'rsi9', 'rsi24', 'MACD5355macddiff', 'MACD5355macddiffslope', 'MACD5355macd', 'MACD5355macdslope', 'MACD5355macdsig', 'MACD5355macdsigslope', 'MACD12269macddiff', 'MACD12269macddiffslope', 'MACD12269macd', 'MACD12269macdslope', 'MACD12269macdsig', 'MACD12269macdsigslope', 'lowTail', 'highTail', 'openTail', 'IntradayBar', 'IntradayRange', 'CloseOverSMA5', 'CloseOverSMA10', 'CloseOverSMA12', 'CloseOverSMA20', 'CloseOverSMA30', 'CloseOverSMA65', 'CloseOverSMA50', 'CloseOverSMA100', 'CloseOverSMA200', 'VolOverSMA5', 'VolOverSMA10', 'VolOverSMA12', 'VolOverSMA20', 'VolOverSMA30', 'VolOverSMA65', 'VolOverSMA50', 'VolOverSMA100', 'VolOverSMA200', 'Ret1day', 'Ret4day', 'Ret8day', 'Ret12day', 'Ret24day', 'Ret72day', 'Ret240day', 'RSC', 'bands_l', 'bands_u', 'ADX', 'cloudA', 'cloudB', 'closeVsIchA', 'closeVsIchB', 'IchAvIchB', 'CondVol_1', 'CondVol_4', 'CondVol_8', 'CondVol_12', 'CondVol_24', 'CondVol_72', 'CondVol_240', 'CV1vCV4', 'CV4vCV8', 'CV8vCV12', 'CV12vCV24', 'CV8vCV24', 'CV24vCV240', 'RSC_VIX', 'RSC_VIX_IV', 'RSC_VIX_real', 'RSC_VIX_IV_real', 'RSC_IV_gar', 'close_spy_corr22', 'close_tnx_corr22', 'vclose_VIX_corr22', 'garch_IV_corr22', 'close_spy_corr65', 'close_tnx_corr65', 'vclose_VIX_corr65', 'garch_IV_corr65', 'close_spy_corr252', 'close_tnx_corr252', 'vclose_VIX_corr252', 'garch_IV_corr252']\n",
       "    (loss): MultiLoss(WrappedTorchmetric(MSELoss()), WrappedTorchmetric(MSELoss()))\n",
       "    (logging_metrics): ModuleList(\n",
       "      (0): SMAPE()\n",
       "      (1): MAE()\n",
       "      (2): RMSE()\n",
       "      (3): MAPE()\n",
       "    )\n",
       "    (input_embeddings): MultiEmbedding(\n",
       "      (embeddings): ModuleDict(\n",
       "        (group): Embedding(179, 29)\n",
       "      )\n",
       "    )\n",
       "    (prescalers): ModuleDict(\n",
       "      (time_idx): Linear(in_features=1, out_features=256, bias=True)\n",
       "      (open): Linear(in_features=1, out_features=256, bias=True)\n",
       "      (high): Linear(in_features=1, out_features=256, bias=True)\n",
       "      (low): Linear(in_features=1, out_features=256, bias=True)\n",
       "      (target_1): Linear(in_features=1, out_features=256, bias=True)\n",
       "      (target_2): Linear(in_features=1, out_features=256, bias=True)\n",
       "      (vopen): Linear(in_features=1, out_features=256, bias=True)\n",
       "      (vhigh): Linear(in_features=1, out_features=256, bias=True)\n",
       "      (vlow): Linear(in_features=1, out_features=256, bias=True)\n",
       "      (VIX): Linear(in_features=1, out_features=256, bias=True)\n",
       "      (SPY): Linear(in_features=1, out_features=256, bias=True)\n",
       "      (TNX): Linear(in_features=1, out_features=256, bias=True)\n",
       "      (rsi14): Linear(in_features=1, out_features=256, bias=True)\n",
       "      (rsi9): Linear(in_features=1, out_features=256, bias=True)\n",
       "      (rsi24): Linear(in_features=1, out_features=256, bias=True)\n",
       "      (MACD5355macddiff): Linear(in_features=1, out_features=256, bias=True)\n",
       "      (MACD5355macddiffslope): Linear(in_features=1, out_features=256, bias=True)\n",
       "      (MACD5355macd): Linear(in_features=1, out_features=256, bias=True)\n",
       "      (MACD5355macdslope): Linear(in_features=1, out_features=256, bias=True)\n",
       "      (MACD5355macdsig): Linear(in_features=1, out_features=256, bias=True)\n",
       "      (MACD5355macdsigslope): Linear(in_features=1, out_features=256, bias=True)\n",
       "      (MACD12269macddiff): Linear(in_features=1, out_features=256, bias=True)\n",
       "      (MACD12269macddiffslope): Linear(in_features=1, out_features=256, bias=True)\n",
       "      (MACD12269macd): Linear(in_features=1, out_features=256, bias=True)\n",
       "      (MACD12269macdslope): Linear(in_features=1, out_features=256, bias=True)\n",
       "      (MACD12269macdsig): Linear(in_features=1, out_features=256, bias=True)\n",
       "      (MACD12269macdsigslope): Linear(in_features=1, out_features=256, bias=True)\n",
       "      (lowTail): Linear(in_features=1, out_features=256, bias=True)\n",
       "      (highTail): Linear(in_features=1, out_features=256, bias=True)\n",
       "      (openTail): Linear(in_features=1, out_features=256, bias=True)\n",
       "      (IntradayBar): Linear(in_features=1, out_features=256, bias=True)\n",
       "      (IntradayRange): Linear(in_features=1, out_features=256, bias=True)\n",
       "      (CloseOverSMA5): Linear(in_features=1, out_features=256, bias=True)\n",
       "      (CloseOverSMA10): Linear(in_features=1, out_features=256, bias=True)\n",
       "      (CloseOverSMA12): Linear(in_features=1, out_features=256, bias=True)\n",
       "      (CloseOverSMA20): Linear(in_features=1, out_features=256, bias=True)\n",
       "      (CloseOverSMA30): Linear(in_features=1, out_features=256, bias=True)\n",
       "      (CloseOverSMA65): Linear(in_features=1, out_features=256, bias=True)\n",
       "      (CloseOverSMA50): Linear(in_features=1, out_features=256, bias=True)\n",
       "      (CloseOverSMA100): Linear(in_features=1, out_features=256, bias=True)\n",
       "      (CloseOverSMA200): Linear(in_features=1, out_features=256, bias=True)\n",
       "      (VolOverSMA5): Linear(in_features=1, out_features=256, bias=True)\n",
       "      (VolOverSMA10): Linear(in_features=1, out_features=256, bias=True)\n",
       "      (VolOverSMA12): Linear(in_features=1, out_features=256, bias=True)\n",
       "      (VolOverSMA20): Linear(in_features=1, out_features=256, bias=True)\n",
       "      (VolOverSMA30): Linear(in_features=1, out_features=256, bias=True)\n",
       "      (VolOverSMA65): Linear(in_features=1, out_features=256, bias=True)\n",
       "      (VolOverSMA50): Linear(in_features=1, out_features=256, bias=True)\n",
       "      (VolOverSMA100): Linear(in_features=1, out_features=256, bias=True)\n",
       "      (VolOverSMA200): Linear(in_features=1, out_features=256, bias=True)\n",
       "      (Ret1day): Linear(in_features=1, out_features=256, bias=True)\n",
       "      (Ret4day): Linear(in_features=1, out_features=256, bias=True)\n",
       "      (Ret8day): Linear(in_features=1, out_features=256, bias=True)\n",
       "      (Ret12day): Linear(in_features=1, out_features=256, bias=True)\n",
       "      (Ret24day): Linear(in_features=1, out_features=256, bias=True)\n",
       "      (Ret72day): Linear(in_features=1, out_features=256, bias=True)\n",
       "      (Ret240day): Linear(in_features=1, out_features=256, bias=True)\n",
       "      (RSC): Linear(in_features=1, out_features=256, bias=True)\n",
       "      (bands_l): Linear(in_features=1, out_features=256, bias=True)\n",
       "      (bands_u): Linear(in_features=1, out_features=256, bias=True)\n",
       "      (ADX): Linear(in_features=1, out_features=256, bias=True)\n",
       "      (cloudA): Linear(in_features=1, out_features=256, bias=True)\n",
       "      (cloudB): Linear(in_features=1, out_features=256, bias=True)\n",
       "      (closeVsIchA): Linear(in_features=1, out_features=256, bias=True)\n",
       "      (closeVsIchB): Linear(in_features=1, out_features=256, bias=True)\n",
       "      (IchAvIchB): Linear(in_features=1, out_features=256, bias=True)\n",
       "      (CondVol_1): Linear(in_features=1, out_features=256, bias=True)\n",
       "      (CondVol_4): Linear(in_features=1, out_features=256, bias=True)\n",
       "      (CondVol_8): Linear(in_features=1, out_features=256, bias=True)\n",
       "      (CondVol_12): Linear(in_features=1, out_features=256, bias=True)\n",
       "      (CondVol_24): Linear(in_features=1, out_features=256, bias=True)\n",
       "      (CondVol_72): Linear(in_features=1, out_features=256, bias=True)\n",
       "      (CondVol_240): Linear(in_features=1, out_features=256, bias=True)\n",
       "      (CV1vCV4): Linear(in_features=1, out_features=256, bias=True)\n",
       "      (CV4vCV8): Linear(in_features=1, out_features=256, bias=True)\n",
       "      (CV8vCV12): Linear(in_features=1, out_features=256, bias=True)\n",
       "      (CV12vCV24): Linear(in_features=1, out_features=256, bias=True)\n",
       "      (CV8vCV24): Linear(in_features=1, out_features=256, bias=True)\n",
       "      (CV24vCV240): Linear(in_features=1, out_features=256, bias=True)\n",
       "      (RSC_VIX): Linear(in_features=1, out_features=256, bias=True)\n",
       "      (RSC_VIX_IV): Linear(in_features=1, out_features=256, bias=True)\n",
       "      (RSC_VIX_real): Linear(in_features=1, out_features=256, bias=True)\n",
       "      (RSC_VIX_IV_real): Linear(in_features=1, out_features=256, bias=True)\n",
       "      (RSC_IV_gar): Linear(in_features=1, out_features=256, bias=True)\n",
       "      (close_spy_corr22): Linear(in_features=1, out_features=256, bias=True)\n",
       "      (close_tnx_corr22): Linear(in_features=1, out_features=256, bias=True)\n",
       "      (vclose_VIX_corr22): Linear(in_features=1, out_features=256, bias=True)\n",
       "      (garch_IV_corr22): Linear(in_features=1, out_features=256, bias=True)\n",
       "      (close_spy_corr65): Linear(in_features=1, out_features=256, bias=True)\n",
       "      (close_tnx_corr65): Linear(in_features=1, out_features=256, bias=True)\n",
       "      (vclose_VIX_corr65): Linear(in_features=1, out_features=256, bias=True)\n",
       "      (garch_IV_corr65): Linear(in_features=1, out_features=256, bias=True)\n",
       "      (close_spy_corr252): Linear(in_features=1, out_features=256, bias=True)\n",
       "      (close_tnx_corr252): Linear(in_features=1, out_features=256, bias=True)\n",
       "      (vclose_VIX_corr252): Linear(in_features=1, out_features=256, bias=True)\n",
       "      (garch_IV_corr252): Linear(in_features=1, out_features=256, bias=True)\n",
       "    )\n",
       "    (static_variable_selection): VariableSelectionNetwork(\n",
       "      (single_variable_grns): ModuleDict(\n",
       "        (group): ResampleNorm(\n",
       "          (resample): TimeDistributedInterpolation()\n",
       "          (gate): Sigmoid()\n",
       "          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (prescalers): ModuleDict()\n",
       "      (softmax): Softmax(dim=-1)\n",
       "    )\n",
       "    (encoder_variable_selection): VariableSelectionNetwork(\n",
       "      (flattened_grn): GatedResidualNetwork(\n",
       "        (resample_norm): ResampleNorm(\n",
       "          (resample): TimeDistributedInterpolation()\n",
       "          (gate): Sigmoid()\n",
       "          (norm): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (fc1): Linear(in_features=24576, out_features=96, bias=True)\n",
       "        (elu): ELU(alpha=1.0)\n",
       "        (context): Linear(in_features=256, out_features=96, bias=False)\n",
       "        (fc2): Linear(in_features=96, out_features=96, bias=True)\n",
       "        (gate_norm): GateAddNorm(\n",
       "          (glu): GatedLinearUnit(\n",
       "            (dropout): Dropout(p=0.35, inplace=False)\n",
       "            (fc): Linear(in_features=96, out_features=192, bias=True)\n",
       "          )\n",
       "          (add_norm): AddNorm(\n",
       "            (norm): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (single_variable_grns): ModuleDict(\n",
       "        (time_idx): GatedResidualNetwork(\n",
       "          (fc1): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (elu): ELU(alpha=1.0)\n",
       "          (fc2): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (gate_norm): GateAddNorm(\n",
       "            (glu): GatedLinearUnit(\n",
       "              (dropout): Dropout(p=0.35, inplace=False)\n",
       "              (fc): Linear(in_features=256, out_features=512, bias=True)\n",
       "            )\n",
       "            (add_norm): AddNorm(\n",
       "              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (open): GatedResidualNetwork(\n",
       "          (fc1): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (elu): ELU(alpha=1.0)\n",
       "          (fc2): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (gate_norm): GateAddNorm(\n",
       "            (glu): GatedLinearUnit(\n",
       "              (dropout): Dropout(p=0.35, inplace=False)\n",
       "              (fc): Linear(in_features=256, out_features=512, bias=True)\n",
       "            )\n",
       "            (add_norm): AddNorm(\n",
       "              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (high): GatedResidualNetwork(\n",
       "          (fc1): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (elu): ELU(alpha=1.0)\n",
       "          (fc2): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (gate_norm): GateAddNorm(\n",
       "            (glu): GatedLinearUnit(\n",
       "              (dropout): Dropout(p=0.35, inplace=False)\n",
       "              (fc): Linear(in_features=256, out_features=512, bias=True)\n",
       "            )\n",
       "            (add_norm): AddNorm(\n",
       "              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (low): GatedResidualNetwork(\n",
       "          (fc1): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (elu): ELU(alpha=1.0)\n",
       "          (fc2): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (gate_norm): GateAddNorm(\n",
       "            (glu): GatedLinearUnit(\n",
       "              (dropout): Dropout(p=0.35, inplace=False)\n",
       "              (fc): Linear(in_features=256, out_features=512, bias=True)\n",
       "            )\n",
       "            (add_norm): AddNorm(\n",
       "              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (target_1): GatedResidualNetwork(\n",
       "          (fc1): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (elu): ELU(alpha=1.0)\n",
       "          (fc2): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (gate_norm): GateAddNorm(\n",
       "            (glu): GatedLinearUnit(\n",
       "              (dropout): Dropout(p=0.35, inplace=False)\n",
       "              (fc): Linear(in_features=256, out_features=512, bias=True)\n",
       "            )\n",
       "            (add_norm): AddNorm(\n",
       "              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (target_2): GatedResidualNetwork(\n",
       "          (fc1): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (elu): ELU(alpha=1.0)\n",
       "          (fc2): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (gate_norm): GateAddNorm(\n",
       "            (glu): GatedLinearUnit(\n",
       "              (dropout): Dropout(p=0.35, inplace=False)\n",
       "              (fc): Linear(in_features=256, out_features=512, bias=True)\n",
       "            )\n",
       "            (add_norm): AddNorm(\n",
       "              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (vopen): GatedResidualNetwork(\n",
       "          (fc1): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (elu): ELU(alpha=1.0)\n",
       "          (fc2): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (gate_norm): GateAddNorm(\n",
       "            (glu): GatedLinearUnit(\n",
       "              (dropout): Dropout(p=0.35, inplace=False)\n",
       "              (fc): Linear(in_features=256, out_features=512, bias=True)\n",
       "            )\n",
       "            (add_norm): AddNorm(\n",
       "              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (vhigh): GatedResidualNetwork(\n",
       "          (fc1): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (elu): ELU(alpha=1.0)\n",
       "          (fc2): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (gate_norm): GateAddNorm(\n",
       "            (glu): GatedLinearUnit(\n",
       "              (dropout): Dropout(p=0.35, inplace=False)\n",
       "              (fc): Linear(in_features=256, out_features=512, bias=True)\n",
       "            )\n",
       "            (add_norm): AddNorm(\n",
       "              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (vlow): GatedResidualNetwork(\n",
       "          (fc1): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (elu): ELU(alpha=1.0)\n",
       "          (fc2): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (gate_norm): GateAddNorm(\n",
       "            (glu): GatedLinearUnit(\n",
       "              (dropout): Dropout(p=0.35, inplace=False)\n",
       "              (fc): Linear(in_features=256, out_features=512, bias=True)\n",
       "            )\n",
       "            (add_norm): AddNorm(\n",
       "              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (VIX): GatedResidualNetwork(\n",
       "          (fc1): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (elu): ELU(alpha=1.0)\n",
       "          (fc2): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (gate_norm): GateAddNorm(\n",
       "            (glu): GatedLinearUnit(\n",
       "              (dropout): Dropout(p=0.35, inplace=False)\n",
       "              (fc): Linear(in_features=256, out_features=512, bias=True)\n",
       "            )\n",
       "            (add_norm): AddNorm(\n",
       "              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (SPY): GatedResidualNetwork(\n",
       "          (fc1): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (elu): ELU(alpha=1.0)\n",
       "          (fc2): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (gate_norm): GateAddNorm(\n",
       "            (glu): GatedLinearUnit(\n",
       "              (dropout): Dropout(p=0.35, inplace=False)\n",
       "              (fc): Linear(in_features=256, out_features=512, bias=True)\n",
       "            )\n",
       "            (add_norm): AddNorm(\n",
       "              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (TNX): GatedResidualNetwork(\n",
       "          (fc1): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (elu): ELU(alpha=1.0)\n",
       "          (fc2): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (gate_norm): GateAddNorm(\n",
       "            (glu): GatedLinearUnit(\n",
       "              (dropout): Dropout(p=0.35, inplace=False)\n",
       "              (fc): Linear(in_features=256, out_features=512, bias=True)\n",
       "            )\n",
       "            (add_norm): AddNorm(\n",
       "              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (rsi14): GatedResidualNetwork(\n",
       "          (fc1): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (elu): ELU(alpha=1.0)\n",
       "          (fc2): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (gate_norm): GateAddNorm(\n",
       "            (glu): GatedLinearUnit(\n",
       "              (dropout): Dropout(p=0.35, inplace=False)\n",
       "              (fc): Linear(in_features=256, out_features=512, bias=True)\n",
       "            )\n",
       "            (add_norm): AddNorm(\n",
       "              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (rsi9): GatedResidualNetwork(\n",
       "          (fc1): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (elu): ELU(alpha=1.0)\n",
       "          (fc2): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (gate_norm): GateAddNorm(\n",
       "            (glu): GatedLinearUnit(\n",
       "              (dropout): Dropout(p=0.35, inplace=False)\n",
       "              (fc): Linear(in_features=256, out_features=512, bias=True)\n",
       "            )\n",
       "            (add_norm): AddNorm(\n",
       "              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (rsi24): GatedResidualNetwork(\n",
       "          (fc1): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (elu): ELU(alpha=1.0)\n",
       "          (fc2): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (gate_norm): GateAddNorm(\n",
       "            (glu): GatedLinearUnit(\n",
       "              (dropout): Dropout(p=0.35, inplace=False)\n",
       "              (fc): Linear(in_features=256, out_features=512, bias=True)\n",
       "            )\n",
       "            (add_norm): AddNorm(\n",
       "              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (MACD5355macddiff): GatedResidualNetwork(\n",
       "          (fc1): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (elu): ELU(alpha=1.0)\n",
       "          (fc2): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (gate_norm): GateAddNorm(\n",
       "            (glu): GatedLinearUnit(\n",
       "              (dropout): Dropout(p=0.35, inplace=False)\n",
       "              (fc): Linear(in_features=256, out_features=512, bias=True)\n",
       "            )\n",
       "            (add_norm): AddNorm(\n",
       "              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (MACD5355macddiffslope): GatedResidualNetwork(\n",
       "          (fc1): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (elu): ELU(alpha=1.0)\n",
       "          (fc2): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (gate_norm): GateAddNorm(\n",
       "            (glu): GatedLinearUnit(\n",
       "              (dropout): Dropout(p=0.35, inplace=False)\n",
       "              (fc): Linear(in_features=256, out_features=512, bias=True)\n",
       "            )\n",
       "            (add_norm): AddNorm(\n",
       "              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (MACD5355macd): GatedResidualNetwork(\n",
       "          (fc1): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (elu): ELU(alpha=1.0)\n",
       "          (fc2): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (gate_norm): GateAddNorm(\n",
       "            (glu): GatedLinearUnit(\n",
       "              (dropout): Dropout(p=0.35, inplace=False)\n",
       "              (fc): Linear(in_features=256, out_features=512, bias=True)\n",
       "            )\n",
       "            (add_norm): AddNorm(\n",
       "              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (MACD5355macdslope): GatedResidualNetwork(\n",
       "          (fc1): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (elu): ELU(alpha=1.0)\n",
       "          (fc2): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (gate_norm): GateAddNorm(\n",
       "            (glu): GatedLinearUnit(\n",
       "              (dropout): Dropout(p=0.35, inplace=False)\n",
       "              (fc): Linear(in_features=256, out_features=512, bias=True)\n",
       "            )\n",
       "            (add_norm): AddNorm(\n",
       "              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (MACD5355macdsig): GatedResidualNetwork(\n",
       "          (fc1): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (elu): ELU(alpha=1.0)\n",
       "          (fc2): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (gate_norm): GateAddNorm(\n",
       "            (glu): GatedLinearUnit(\n",
       "              (dropout): Dropout(p=0.35, inplace=False)\n",
       "              (fc): Linear(in_features=256, out_features=512, bias=True)\n",
       "            )\n",
       "            (add_norm): AddNorm(\n",
       "              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (MACD5355macdsigslope): GatedResidualNetwork(\n",
       "          (fc1): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (elu): ELU(alpha=1.0)\n",
       "          (fc2): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (gate_norm): GateAddNorm(\n",
       "            (glu): GatedLinearUnit(\n",
       "              (dropout): Dropout(p=0.35, inplace=False)\n",
       "              (fc): Linear(in_features=256, out_features=512, bias=True)\n",
       "            )\n",
       "            (add_norm): AddNorm(\n",
       "              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (MACD12269macddiff): GatedResidualNetwork(\n",
       "          (fc1): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (elu): ELU(alpha=1.0)\n",
       "          (fc2): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (gate_norm): GateAddNorm(\n",
       "            (glu): GatedLinearUnit(\n",
       "              (dropout): Dropout(p=0.35, inplace=False)\n",
       "              (fc): Linear(in_features=256, out_features=512, bias=True)\n",
       "            )\n",
       "            (add_norm): AddNorm(\n",
       "              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (MACD12269macddiffslope): GatedResidualNetwork(\n",
       "          (fc1): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (elu): ELU(alpha=1.0)\n",
       "          (fc2): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (gate_norm): GateAddNorm(\n",
       "            (glu): GatedLinearUnit(\n",
       "              (dropout): Dropout(p=0.35, inplace=False)\n",
       "              (fc): Linear(in_features=256, out_features=512, bias=True)\n",
       "            )\n",
       "            (add_norm): AddNorm(\n",
       "              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (MACD12269macd): GatedResidualNetwork(\n",
       "          (fc1): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (elu): ELU(alpha=1.0)\n",
       "          (fc2): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (gate_norm): GateAddNorm(\n",
       "            (glu): GatedLinearUnit(\n",
       "              (dropout): Dropout(p=0.35, inplace=False)\n",
       "              (fc): Linear(in_features=256, out_features=512, bias=True)\n",
       "            )\n",
       "            (add_norm): AddNorm(\n",
       "              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (MACD12269macdslope): GatedResidualNetwork(\n",
       "          (fc1): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (elu): ELU(alpha=1.0)\n",
       "          (fc2): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (gate_norm): GateAddNorm(\n",
       "            (glu): GatedLinearUnit(\n",
       "              (dropout): Dropout(p=0.35, inplace=False)\n",
       "              (fc): Linear(in_features=256, out_features=512, bias=True)\n",
       "            )\n",
       "            (add_norm): AddNorm(\n",
       "              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (MACD12269macdsig): GatedResidualNetwork(\n",
       "          (fc1): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (elu): ELU(alpha=1.0)\n",
       "          (fc2): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (gate_norm): GateAddNorm(\n",
       "            (glu): GatedLinearUnit(\n",
       "              (dropout): Dropout(p=0.35, inplace=False)\n",
       "              (fc): Linear(in_features=256, out_features=512, bias=True)\n",
       "            )\n",
       "            (add_norm): AddNorm(\n",
       "              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (MACD12269macdsigslope): GatedResidualNetwork(\n",
       "          (fc1): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (elu): ELU(alpha=1.0)\n",
       "          (fc2): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (gate_norm): GateAddNorm(\n",
       "            (glu): GatedLinearUnit(\n",
       "              (dropout): Dropout(p=0.35, inplace=False)\n",
       "              (fc): Linear(in_features=256, out_features=512, bias=True)\n",
       "            )\n",
       "            (add_norm): AddNorm(\n",
       "              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (lowTail): GatedResidualNetwork(\n",
       "          (fc1): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (elu): ELU(alpha=1.0)\n",
       "          (fc2): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (gate_norm): GateAddNorm(\n",
       "            (glu): GatedLinearUnit(\n",
       "              (dropout): Dropout(p=0.35, inplace=False)\n",
       "              (fc): Linear(in_features=256, out_features=512, bias=True)\n",
       "            )\n",
       "            (add_norm): AddNorm(\n",
       "              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (highTail): GatedResidualNetwork(\n",
       "          (fc1): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (elu): ELU(alpha=1.0)\n",
       "          (fc2): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (gate_norm): GateAddNorm(\n",
       "            (glu): GatedLinearUnit(\n",
       "              (dropout): Dropout(p=0.35, inplace=False)\n",
       "              (fc): Linear(in_features=256, out_features=512, bias=True)\n",
       "            )\n",
       "            (add_norm): AddNorm(\n",
       "              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (openTail): GatedResidualNetwork(\n",
       "          (fc1): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (elu): ELU(alpha=1.0)\n",
       "          (fc2): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (gate_norm): GateAddNorm(\n",
       "            (glu): GatedLinearUnit(\n",
       "              (dropout): Dropout(p=0.35, inplace=False)\n",
       "              (fc): Linear(in_features=256, out_features=512, bias=True)\n",
       "            )\n",
       "            (add_norm): AddNorm(\n",
       "              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (IntradayBar): GatedResidualNetwork(\n",
       "          (fc1): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (elu): ELU(alpha=1.0)\n",
       "          (fc2): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (gate_norm): GateAddNorm(\n",
       "            (glu): GatedLinearUnit(\n",
       "              (dropout): Dropout(p=0.35, inplace=False)\n",
       "              (fc): Linear(in_features=256, out_features=512, bias=True)\n",
       "            )\n",
       "            (add_norm): AddNorm(\n",
       "              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (IntradayRange): GatedResidualNetwork(\n",
       "          (fc1): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (elu): ELU(alpha=1.0)\n",
       "          (fc2): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (gate_norm): GateAddNorm(\n",
       "            (glu): GatedLinearUnit(\n",
       "              (dropout): Dropout(p=0.35, inplace=False)\n",
       "              (fc): Linear(in_features=256, out_features=512, bias=True)\n",
       "            )\n",
       "            (add_norm): AddNorm(\n",
       "              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (CloseOverSMA5): GatedResidualNetwork(\n",
       "          (fc1): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (elu): ELU(alpha=1.0)\n",
       "          (fc2): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (gate_norm): GateAddNorm(\n",
       "            (glu): GatedLinearUnit(\n",
       "              (dropout): Dropout(p=0.35, inplace=False)\n",
       "              (fc): Linear(in_features=256, out_features=512, bias=True)\n",
       "            )\n",
       "            (add_norm): AddNorm(\n",
       "              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (CloseOverSMA10): GatedResidualNetwork(\n",
       "          (fc1): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (elu): ELU(alpha=1.0)\n",
       "          (fc2): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (gate_norm): GateAddNorm(\n",
       "            (glu): GatedLinearUnit(\n",
       "              (dropout): Dropout(p=0.35, inplace=False)\n",
       "              (fc): Linear(in_features=256, out_features=512, bias=True)\n",
       "            )\n",
       "            (add_norm): AddNorm(\n",
       "              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (CloseOverSMA12): GatedResidualNetwork(\n",
       "          (fc1): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (elu): ELU(alpha=1.0)\n",
       "          (fc2): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (gate_norm): GateAddNorm(\n",
       "            (glu): GatedLinearUnit(\n",
       "              (dropout): Dropout(p=0.35, inplace=False)\n",
       "              (fc): Linear(in_features=256, out_features=512, bias=True)\n",
       "            )\n",
       "            (add_norm): AddNorm(\n",
       "              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (CloseOverSMA20): GatedResidualNetwork(\n",
       "          (fc1): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (elu): ELU(alpha=1.0)\n",
       "          (fc2): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (gate_norm): GateAddNorm(\n",
       "            (glu): GatedLinearUnit(\n",
       "              (dropout): Dropout(p=0.35, inplace=False)\n",
       "              (fc): Linear(in_features=256, out_features=512, bias=True)\n",
       "            )\n",
       "            (add_norm): AddNorm(\n",
       "              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (CloseOverSMA30): GatedResidualNetwork(\n",
       "          (fc1): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (elu): ELU(alpha=1.0)\n",
       "          (fc2): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (gate_norm): GateAddNorm(\n",
       "            (glu): GatedLinearUnit(\n",
       "              (dropout): Dropout(p=0.35, inplace=False)\n",
       "              (fc): Linear(in_features=256, out_features=512, bias=True)\n",
       "            )\n",
       "            (add_norm): AddNorm(\n",
       "              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (CloseOverSMA65): GatedResidualNetwork(\n",
       "          (fc1): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (elu): ELU(alpha=1.0)\n",
       "          (fc2): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (gate_norm): GateAddNorm(\n",
       "            (glu): GatedLinearUnit(\n",
       "              (dropout): Dropout(p=0.35, inplace=False)\n",
       "              (fc): Linear(in_features=256, out_features=512, bias=True)\n",
       "            )\n",
       "            (add_norm): AddNorm(\n",
       "              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (CloseOverSMA50): GatedResidualNetwork(\n",
       "          (fc1): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (elu): ELU(alpha=1.0)\n",
       "          (fc2): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (gate_norm): GateAddNorm(\n",
       "            (glu): GatedLinearUnit(\n",
       "              (dropout): Dropout(p=0.35, inplace=False)\n",
       "              (fc): Linear(in_features=256, out_features=512, bias=True)\n",
       "            )\n",
       "            (add_norm): AddNorm(\n",
       "              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (CloseOverSMA100): GatedResidualNetwork(\n",
       "          (fc1): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (elu): ELU(alpha=1.0)\n",
       "          (fc2): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (gate_norm): GateAddNorm(\n",
       "            (glu): GatedLinearUnit(\n",
       "              (dropout): Dropout(p=0.35, inplace=False)\n",
       "              (fc): Linear(in_features=256, out_features=512, bias=True)\n",
       "            )\n",
       "            (add_norm): AddNorm(\n",
       "              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (CloseOverSMA200): GatedResidualNetwork(\n",
       "          (fc1): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (elu): ELU(alpha=1.0)\n",
       "          (fc2): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (gate_norm): GateAddNorm(\n",
       "            (glu): GatedLinearUnit(\n",
       "              (dropout): Dropout(p=0.35, inplace=False)\n",
       "              (fc): Linear(in_features=256, out_features=512, bias=True)\n",
       "            )\n",
       "            (add_norm): AddNorm(\n",
       "              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (VolOverSMA5): GatedResidualNetwork(\n",
       "          (fc1): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (elu): ELU(alpha=1.0)\n",
       "          (fc2): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (gate_norm): GateAddNorm(\n",
       "            (glu): GatedLinearUnit(\n",
       "              (dropout): Dropout(p=0.35, inplace=False)\n",
       "              (fc): Linear(in_features=256, out_features=512, bias=True)\n",
       "            )\n",
       "            (add_norm): AddNorm(\n",
       "              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (VolOverSMA10): GatedResidualNetwork(\n",
       "          (fc1): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (elu): ELU(alpha=1.0)\n",
       "          (fc2): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (gate_norm): GateAddNorm(\n",
       "            (glu): GatedLinearUnit(\n",
       "              (dropout): Dropout(p=0.35, inplace=False)\n",
       "              (fc): Linear(in_features=256, out_features=512, bias=True)\n",
       "            )\n",
       "            (add_norm): AddNorm(\n",
       "              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (VolOverSMA12): GatedResidualNetwork(\n",
       "          (fc1): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (elu): ELU(alpha=1.0)\n",
       "          (fc2): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (gate_norm): GateAddNorm(\n",
       "            (glu): GatedLinearUnit(\n",
       "              (dropout): Dropout(p=0.35, inplace=False)\n",
       "              (fc): Linear(in_features=256, out_features=512, bias=True)\n",
       "            )\n",
       "            (add_norm): AddNorm(\n",
       "              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (VolOverSMA20): GatedResidualNetwork(\n",
       "          (fc1): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (elu): ELU(alpha=1.0)\n",
       "          (fc2): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (gate_norm): GateAddNorm(\n",
       "            (glu): GatedLinearUnit(\n",
       "              (dropout): Dropout(p=0.35, inplace=False)\n",
       "              (fc): Linear(in_features=256, out_features=512, bias=True)\n",
       "            )\n",
       "            (add_norm): AddNorm(\n",
       "              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (VolOverSMA30): GatedResidualNetwork(\n",
       "          (fc1): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (elu): ELU(alpha=1.0)\n",
       "          (fc2): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (gate_norm): GateAddNorm(\n",
       "            (glu): GatedLinearUnit(\n",
       "              (dropout): Dropout(p=0.35, inplace=False)\n",
       "              (fc): Linear(in_features=256, out_features=512, bias=True)\n",
       "            )\n",
       "            (add_norm): AddNorm(\n",
       "              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (VolOverSMA65): GatedResidualNetwork(\n",
       "          (fc1): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (elu): ELU(alpha=1.0)\n",
       "          (fc2): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (gate_norm): GateAddNorm(\n",
       "            (glu): GatedLinearUnit(\n",
       "              (dropout): Dropout(p=0.35, inplace=False)\n",
       "              (fc): Linear(in_features=256, out_features=512, bias=True)\n",
       "            )\n",
       "            (add_norm): AddNorm(\n",
       "              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (VolOverSMA50): GatedResidualNetwork(\n",
       "          (fc1): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (elu): ELU(alpha=1.0)\n",
       "          (fc2): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (gate_norm): GateAddNorm(\n",
       "            (glu): GatedLinearUnit(\n",
       "              (dropout): Dropout(p=0.35, inplace=False)\n",
       "              (fc): Linear(in_features=256, out_features=512, bias=True)\n",
       "            )\n",
       "            (add_norm): AddNorm(\n",
       "              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (VolOverSMA100): GatedResidualNetwork(\n",
       "          (fc1): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (elu): ELU(alpha=1.0)\n",
       "          (fc2): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (gate_norm): GateAddNorm(\n",
       "            (glu): GatedLinearUnit(\n",
       "              (dropout): Dropout(p=0.35, inplace=False)\n",
       "              (fc): Linear(in_features=256, out_features=512, bias=True)\n",
       "            )\n",
       "            (add_norm): AddNorm(\n",
       "              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (VolOverSMA200): GatedResidualNetwork(\n",
       "          (fc1): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (elu): ELU(alpha=1.0)\n",
       "          (fc2): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (gate_norm): GateAddNorm(\n",
       "            (glu): GatedLinearUnit(\n",
       "              (dropout): Dropout(p=0.35, inplace=False)\n",
       "              (fc): Linear(in_features=256, out_features=512, bias=True)\n",
       "            )\n",
       "            (add_norm): AddNorm(\n",
       "              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (Ret1day): GatedResidualNetwork(\n",
       "          (fc1): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (elu): ELU(alpha=1.0)\n",
       "          (fc2): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (gate_norm): GateAddNorm(\n",
       "            (glu): GatedLinearUnit(\n",
       "              (dropout): Dropout(p=0.35, inplace=False)\n",
       "              (fc): Linear(in_features=256, out_features=512, bias=True)\n",
       "            )\n",
       "            (add_norm): AddNorm(\n",
       "              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (Ret4day): GatedResidualNetwork(\n",
       "          (fc1): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (elu): ELU(alpha=1.0)\n",
       "          (fc2): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (gate_norm): GateAddNorm(\n",
       "            (glu): GatedLinearUnit(\n",
       "              (dropout): Dropout(p=0.35, inplace=False)\n",
       "              (fc): Linear(in_features=256, out_features=512, bias=True)\n",
       "            )\n",
       "            (add_norm): AddNorm(\n",
       "              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (Ret8day): GatedResidualNetwork(\n",
       "          (fc1): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (elu): ELU(alpha=1.0)\n",
       "          (fc2): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (gate_norm): GateAddNorm(\n",
       "            (glu): GatedLinearUnit(\n",
       "              (dropout): Dropout(p=0.35, inplace=False)\n",
       "              (fc): Linear(in_features=256, out_features=512, bias=True)\n",
       "            )\n",
       "            (add_norm): AddNorm(\n",
       "              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (Ret12day): GatedResidualNetwork(\n",
       "          (fc1): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (elu): ELU(alpha=1.0)\n",
       "          (fc2): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (gate_norm): GateAddNorm(\n",
       "            (glu): GatedLinearUnit(\n",
       "              (dropout): Dropout(p=0.35, inplace=False)\n",
       "              (fc): Linear(in_features=256, out_features=512, bias=True)\n",
       "            )\n",
       "            (add_norm): AddNorm(\n",
       "              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (Ret24day): GatedResidualNetwork(\n",
       "          (fc1): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (elu): ELU(alpha=1.0)\n",
       "          (fc2): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (gate_norm): GateAddNorm(\n",
       "            (glu): GatedLinearUnit(\n",
       "              (dropout): Dropout(p=0.35, inplace=False)\n",
       "              (fc): Linear(in_features=256, out_features=512, bias=True)\n",
       "            )\n",
       "            (add_norm): AddNorm(\n",
       "              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (Ret72day): GatedResidualNetwork(\n",
       "          (fc1): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (elu): ELU(alpha=1.0)\n",
       "          (fc2): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (gate_norm): GateAddNorm(\n",
       "            (glu): GatedLinearUnit(\n",
       "              (dropout): Dropout(p=0.35, inplace=False)\n",
       "              (fc): Linear(in_features=256, out_features=512, bias=True)\n",
       "            )\n",
       "            (add_norm): AddNorm(\n",
       "              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (Ret240day): GatedResidualNetwork(\n",
       "          (fc1): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (elu): ELU(alpha=1.0)\n",
       "          (fc2): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (gate_norm): GateAddNorm(\n",
       "            (glu): GatedLinearUnit(\n",
       "              (dropout): Dropout(p=0.35, inplace=False)\n",
       "              (fc): Linear(in_features=256, out_features=512, bias=True)\n",
       "            )\n",
       "            (add_norm): AddNorm(\n",
       "              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (RSC): GatedResidualNetwork(\n",
       "          (fc1): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (elu): ELU(alpha=1.0)\n",
       "          (fc2): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (gate_norm): GateAddNorm(\n",
       "            (glu): GatedLinearUnit(\n",
       "              (dropout): Dropout(p=0.35, inplace=False)\n",
       "              (fc): Linear(in_features=256, out_features=512, bias=True)\n",
       "            )\n",
       "            (add_norm): AddNorm(\n",
       "              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (bands_l): GatedResidualNetwork(\n",
       "          (fc1): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (elu): ELU(alpha=1.0)\n",
       "          (fc2): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (gate_norm): GateAddNorm(\n",
       "            (glu): GatedLinearUnit(\n",
       "              (dropout): Dropout(p=0.35, inplace=False)\n",
       "              (fc): Linear(in_features=256, out_features=512, bias=True)\n",
       "            )\n",
       "            (add_norm): AddNorm(\n",
       "              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (bands_u): GatedResidualNetwork(\n",
       "          (fc1): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (elu): ELU(alpha=1.0)\n",
       "          (fc2): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (gate_norm): GateAddNorm(\n",
       "            (glu): GatedLinearUnit(\n",
       "              (dropout): Dropout(p=0.35, inplace=False)\n",
       "              (fc): Linear(in_features=256, out_features=512, bias=True)\n",
       "            )\n",
       "            (add_norm): AddNorm(\n",
       "              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (ADX): GatedResidualNetwork(\n",
       "          (fc1): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (elu): ELU(alpha=1.0)\n",
       "          (fc2): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (gate_norm): GateAddNorm(\n",
       "            (glu): GatedLinearUnit(\n",
       "              (dropout): Dropout(p=0.35, inplace=False)\n",
       "              (fc): Linear(in_features=256, out_features=512, bias=True)\n",
       "            )\n",
       "            (add_norm): AddNorm(\n",
       "              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (cloudA): GatedResidualNetwork(\n",
       "          (fc1): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (elu): ELU(alpha=1.0)\n",
       "          (fc2): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (gate_norm): GateAddNorm(\n",
       "            (glu): GatedLinearUnit(\n",
       "              (dropout): Dropout(p=0.35, inplace=False)\n",
       "              (fc): Linear(in_features=256, out_features=512, bias=True)\n",
       "            )\n",
       "            (add_norm): AddNorm(\n",
       "              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (cloudB): GatedResidualNetwork(\n",
       "          (fc1): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (elu): ELU(alpha=1.0)\n",
       "          (fc2): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (gate_norm): GateAddNorm(\n",
       "            (glu): GatedLinearUnit(\n",
       "              (dropout): Dropout(p=0.35, inplace=False)\n",
       "              (fc): Linear(in_features=256, out_features=512, bias=True)\n",
       "            )\n",
       "            (add_norm): AddNorm(\n",
       "              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (closeVsIchA): GatedResidualNetwork(\n",
       "          (fc1): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (elu): ELU(alpha=1.0)\n",
       "          (fc2): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (gate_norm): GateAddNorm(\n",
       "            (glu): GatedLinearUnit(\n",
       "              (dropout): Dropout(p=0.35, inplace=False)\n",
       "              (fc): Linear(in_features=256, out_features=512, bias=True)\n",
       "            )\n",
       "            (add_norm): AddNorm(\n",
       "              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (closeVsIchB): GatedResidualNetwork(\n",
       "          (fc1): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (elu): ELU(alpha=1.0)\n",
       "          (fc2): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (gate_norm): GateAddNorm(\n",
       "            (glu): GatedLinearUnit(\n",
       "              (dropout): Dropout(p=0.35, inplace=False)\n",
       "              (fc): Linear(in_features=256, out_features=512, bias=True)\n",
       "            )\n",
       "            (add_norm): AddNorm(\n",
       "              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (IchAvIchB): GatedResidualNetwork(\n",
       "          (fc1): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (elu): ELU(alpha=1.0)\n",
       "          (fc2): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (gate_norm): GateAddNorm(\n",
       "            (glu): GatedLinearUnit(\n",
       "              (dropout): Dropout(p=0.35, inplace=False)\n",
       "              (fc): Linear(in_features=256, out_features=512, bias=True)\n",
       "            )\n",
       "            (add_norm): AddNorm(\n",
       "              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (CondVol_1): GatedResidualNetwork(\n",
       "          (fc1): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (elu): ELU(alpha=1.0)\n",
       "          (fc2): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (gate_norm): GateAddNorm(\n",
       "            (glu): GatedLinearUnit(\n",
       "              (dropout): Dropout(p=0.35, inplace=False)\n",
       "              (fc): Linear(in_features=256, out_features=512, bias=True)\n",
       "            )\n",
       "            (add_norm): AddNorm(\n",
       "              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (CondVol_4): GatedResidualNetwork(\n",
       "          (fc1): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (elu): ELU(alpha=1.0)\n",
       "          (fc2): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (gate_norm): GateAddNorm(\n",
       "            (glu): GatedLinearUnit(\n",
       "              (dropout): Dropout(p=0.35, inplace=False)\n",
       "              (fc): Linear(in_features=256, out_features=512, bias=True)\n",
       "            )\n",
       "            (add_norm): AddNorm(\n",
       "              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (CondVol_8): GatedResidualNetwork(\n",
       "          (fc1): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (elu): ELU(alpha=1.0)\n",
       "          (fc2): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (gate_norm): GateAddNorm(\n",
       "            (glu): GatedLinearUnit(\n",
       "              (dropout): Dropout(p=0.35, inplace=False)\n",
       "              (fc): Linear(in_features=256, out_features=512, bias=True)\n",
       "            )\n",
       "            (add_norm): AddNorm(\n",
       "              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (CondVol_12): GatedResidualNetwork(\n",
       "          (fc1): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (elu): ELU(alpha=1.0)\n",
       "          (fc2): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (gate_norm): GateAddNorm(\n",
       "            (glu): GatedLinearUnit(\n",
       "              (dropout): Dropout(p=0.35, inplace=False)\n",
       "              (fc): Linear(in_features=256, out_features=512, bias=True)\n",
       "            )\n",
       "            (add_norm): AddNorm(\n",
       "              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (CondVol_24): GatedResidualNetwork(\n",
       "          (fc1): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (elu): ELU(alpha=1.0)\n",
       "          (fc2): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (gate_norm): GateAddNorm(\n",
       "            (glu): GatedLinearUnit(\n",
       "              (dropout): Dropout(p=0.35, inplace=False)\n",
       "              (fc): Linear(in_features=256, out_features=512, bias=True)\n",
       "            )\n",
       "            (add_norm): AddNorm(\n",
       "              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (CondVol_72): GatedResidualNetwork(\n",
       "          (fc1): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (elu): ELU(alpha=1.0)\n",
       "          (fc2): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (gate_norm): GateAddNorm(\n",
       "            (glu): GatedLinearUnit(\n",
       "              (dropout): Dropout(p=0.35, inplace=False)\n",
       "              (fc): Linear(in_features=256, out_features=512, bias=True)\n",
       "            )\n",
       "            (add_norm): AddNorm(\n",
       "              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (CondVol_240): GatedResidualNetwork(\n",
       "          (fc1): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (elu): ELU(alpha=1.0)\n",
       "          (fc2): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (gate_norm): GateAddNorm(\n",
       "            (glu): GatedLinearUnit(\n",
       "              (dropout): Dropout(p=0.35, inplace=False)\n",
       "              (fc): Linear(in_features=256, out_features=512, bias=True)\n",
       "            )\n",
       "            (add_norm): AddNorm(\n",
       "              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (CV1vCV4): GatedResidualNetwork(\n",
       "          (fc1): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (elu): ELU(alpha=1.0)\n",
       "          (fc2): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (gate_norm): GateAddNorm(\n",
       "            (glu): GatedLinearUnit(\n",
       "              (dropout): Dropout(p=0.35, inplace=False)\n",
       "              (fc): Linear(in_features=256, out_features=512, bias=True)\n",
       "            )\n",
       "            (add_norm): AddNorm(\n",
       "              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (CV4vCV8): GatedResidualNetwork(\n",
       "          (fc1): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (elu): ELU(alpha=1.0)\n",
       "          (fc2): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (gate_norm): GateAddNorm(\n",
       "            (glu): GatedLinearUnit(\n",
       "              (dropout): Dropout(p=0.35, inplace=False)\n",
       "              (fc): Linear(in_features=256, out_features=512, bias=True)\n",
       "            )\n",
       "            (add_norm): AddNorm(\n",
       "              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (CV8vCV12): GatedResidualNetwork(\n",
       "          (fc1): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (elu): ELU(alpha=1.0)\n",
       "          (fc2): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (gate_norm): GateAddNorm(\n",
       "            (glu): GatedLinearUnit(\n",
       "              (dropout): Dropout(p=0.35, inplace=False)\n",
       "              (fc): Linear(in_features=256, out_features=512, bias=True)\n",
       "            )\n",
       "            (add_norm): AddNorm(\n",
       "              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (CV12vCV24): GatedResidualNetwork(\n",
       "          (fc1): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (elu): ELU(alpha=1.0)\n",
       "          (fc2): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (gate_norm): GateAddNorm(\n",
       "            (glu): GatedLinearUnit(\n",
       "              (dropout): Dropout(p=0.35, inplace=False)\n",
       "              (fc): Linear(in_features=256, out_features=512, bias=True)\n",
       "            )\n",
       "            (add_norm): AddNorm(\n",
       "              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (CV8vCV24): GatedResidualNetwork(\n",
       "          (fc1): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (elu): ELU(alpha=1.0)\n",
       "          (fc2): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (gate_norm): GateAddNorm(\n",
       "            (glu): GatedLinearUnit(\n",
       "              (dropout): Dropout(p=0.35, inplace=False)\n",
       "              (fc): Linear(in_features=256, out_features=512, bias=True)\n",
       "            )\n",
       "            (add_norm): AddNorm(\n",
       "              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (CV24vCV240): GatedResidualNetwork(\n",
       "          (fc1): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (elu): ELU(alpha=1.0)\n",
       "          (fc2): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (gate_norm): GateAddNorm(\n",
       "            (glu): GatedLinearUnit(\n",
       "              (dropout): Dropout(p=0.35, inplace=False)\n",
       "              (fc): Linear(in_features=256, out_features=512, bias=True)\n",
       "            )\n",
       "            (add_norm): AddNorm(\n",
       "              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (RSC_VIX): GatedResidualNetwork(\n",
       "          (fc1): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (elu): ELU(alpha=1.0)\n",
       "          (fc2): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (gate_norm): GateAddNorm(\n",
       "            (glu): GatedLinearUnit(\n",
       "              (dropout): Dropout(p=0.35, inplace=False)\n",
       "              (fc): Linear(in_features=256, out_features=512, bias=True)\n",
       "            )\n",
       "            (add_norm): AddNorm(\n",
       "              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (RSC_VIX_IV): GatedResidualNetwork(\n",
       "          (fc1): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (elu): ELU(alpha=1.0)\n",
       "          (fc2): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (gate_norm): GateAddNorm(\n",
       "            (glu): GatedLinearUnit(\n",
       "              (dropout): Dropout(p=0.35, inplace=False)\n",
       "              (fc): Linear(in_features=256, out_features=512, bias=True)\n",
       "            )\n",
       "            (add_norm): AddNorm(\n",
       "              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (RSC_VIX_real): GatedResidualNetwork(\n",
       "          (fc1): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (elu): ELU(alpha=1.0)\n",
       "          (fc2): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (gate_norm): GateAddNorm(\n",
       "            (glu): GatedLinearUnit(\n",
       "              (dropout): Dropout(p=0.35, inplace=False)\n",
       "              (fc): Linear(in_features=256, out_features=512, bias=True)\n",
       "            )\n",
       "            (add_norm): AddNorm(\n",
       "              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (RSC_VIX_IV_real): GatedResidualNetwork(\n",
       "          (fc1): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (elu): ELU(alpha=1.0)\n",
       "          (fc2): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (gate_norm): GateAddNorm(\n",
       "            (glu): GatedLinearUnit(\n",
       "              (dropout): Dropout(p=0.35, inplace=False)\n",
       "              (fc): Linear(in_features=256, out_features=512, bias=True)\n",
       "            )\n",
       "            (add_norm): AddNorm(\n",
       "              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (RSC_IV_gar): GatedResidualNetwork(\n",
       "          (fc1): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (elu): ELU(alpha=1.0)\n",
       "          (fc2): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (gate_norm): GateAddNorm(\n",
       "            (glu): GatedLinearUnit(\n",
       "              (dropout): Dropout(p=0.35, inplace=False)\n",
       "              (fc): Linear(in_features=256, out_features=512, bias=True)\n",
       "            )\n",
       "            (add_norm): AddNorm(\n",
       "              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (close_spy_corr22): GatedResidualNetwork(\n",
       "          (fc1): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (elu): ELU(alpha=1.0)\n",
       "          (fc2): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (gate_norm): GateAddNorm(\n",
       "            (glu): GatedLinearUnit(\n",
       "              (dropout): Dropout(p=0.35, inplace=False)\n",
       "              (fc): Linear(in_features=256, out_features=512, bias=True)\n",
       "            )\n",
       "            (add_norm): AddNorm(\n",
       "              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (close_tnx_corr22): GatedResidualNetwork(\n",
       "          (fc1): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (elu): ELU(alpha=1.0)\n",
       "          (fc2): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (gate_norm): GateAddNorm(\n",
       "            (glu): GatedLinearUnit(\n",
       "              (dropout): Dropout(p=0.35, inplace=False)\n",
       "              (fc): Linear(in_features=256, out_features=512, bias=True)\n",
       "            )\n",
       "            (add_norm): AddNorm(\n",
       "              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (vclose_VIX_corr22): GatedResidualNetwork(\n",
       "          (fc1): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (elu): ELU(alpha=1.0)\n",
       "          (fc2): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (gate_norm): GateAddNorm(\n",
       "            (glu): GatedLinearUnit(\n",
       "              (dropout): Dropout(p=0.35, inplace=False)\n",
       "              (fc): Linear(in_features=256, out_features=512, bias=True)\n",
       "            )\n",
       "            (add_norm): AddNorm(\n",
       "              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (garch_IV_corr22): GatedResidualNetwork(\n",
       "          (fc1): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (elu): ELU(alpha=1.0)\n",
       "          (fc2): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (gate_norm): GateAddNorm(\n",
       "            (glu): GatedLinearUnit(\n",
       "              (dropout): Dropout(p=0.35, inplace=False)\n",
       "              (fc): Linear(in_features=256, out_features=512, bias=True)\n",
       "            )\n",
       "            (add_norm): AddNorm(\n",
       "              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (close_spy_corr65): GatedResidualNetwork(\n",
       "          (fc1): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (elu): ELU(alpha=1.0)\n",
       "          (fc2): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (gate_norm): GateAddNorm(\n",
       "            (glu): GatedLinearUnit(\n",
       "              (dropout): Dropout(p=0.35, inplace=False)\n",
       "              (fc): Linear(in_features=256, out_features=512, bias=True)\n",
       "            )\n",
       "            (add_norm): AddNorm(\n",
       "              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (close_tnx_corr65): GatedResidualNetwork(\n",
       "          (fc1): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (elu): ELU(alpha=1.0)\n",
       "          (fc2): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (gate_norm): GateAddNorm(\n",
       "            (glu): GatedLinearUnit(\n",
       "              (dropout): Dropout(p=0.35, inplace=False)\n",
       "              (fc): Linear(in_features=256, out_features=512, bias=True)\n",
       "            )\n",
       "            (add_norm): AddNorm(\n",
       "              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (vclose_VIX_corr65): GatedResidualNetwork(\n",
       "          (fc1): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (elu): ELU(alpha=1.0)\n",
       "          (fc2): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (gate_norm): GateAddNorm(\n",
       "            (glu): GatedLinearUnit(\n",
       "              (dropout): Dropout(p=0.35, inplace=False)\n",
       "              (fc): Linear(in_features=256, out_features=512, bias=True)\n",
       "            )\n",
       "            (add_norm): AddNorm(\n",
       "              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (garch_IV_corr65): GatedResidualNetwork(\n",
       "          (fc1): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (elu): ELU(alpha=1.0)\n",
       "          (fc2): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (gate_norm): GateAddNorm(\n",
       "            (glu): GatedLinearUnit(\n",
       "              (dropout): Dropout(p=0.35, inplace=False)\n",
       "              (fc): Linear(in_features=256, out_features=512, bias=True)\n",
       "            )\n",
       "            (add_norm): AddNorm(\n",
       "              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (close_spy_corr252): GatedResidualNetwork(\n",
       "          (fc1): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (elu): ELU(alpha=1.0)\n",
       "          (fc2): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (gate_norm): GateAddNorm(\n",
       "            (glu): GatedLinearUnit(\n",
       "              (dropout): Dropout(p=0.35, inplace=False)\n",
       "              (fc): Linear(in_features=256, out_features=512, bias=True)\n",
       "            )\n",
       "            (add_norm): AddNorm(\n",
       "              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (close_tnx_corr252): GatedResidualNetwork(\n",
       "          (fc1): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (elu): ELU(alpha=1.0)\n",
       "          (fc2): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (gate_norm): GateAddNorm(\n",
       "            (glu): GatedLinearUnit(\n",
       "              (dropout): Dropout(p=0.35, inplace=False)\n",
       "              (fc): Linear(in_features=256, out_features=512, bias=True)\n",
       "            )\n",
       "            (add_norm): AddNorm(\n",
       "              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (vclose_VIX_corr252): GatedResidualNetwork(\n",
       "          (fc1): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (elu): ELU(alpha=1.0)\n",
       "          (fc2): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (gate_norm): GateAddNorm(\n",
       "            (glu): GatedLinearUnit(\n",
       "              (dropout): Dropout(p=0.35, inplace=False)\n",
       "              (fc): Linear(in_features=256, out_features=512, bias=True)\n",
       "            )\n",
       "            (add_norm): AddNorm(\n",
       "              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (garch_IV_corr252): GatedResidualNetwork(\n",
       "          (fc1): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (elu): ELU(alpha=1.0)\n",
       "          (fc2): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (gate_norm): GateAddNorm(\n",
       "            (glu): GatedLinearUnit(\n",
       "              (dropout): Dropout(p=0.35, inplace=False)\n",
       "              (fc): Linear(in_features=256, out_features=512, bias=True)\n",
       "            )\n",
       "            (add_norm): AddNorm(\n",
       "              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (prescalers): ModuleDict(\n",
       "        (time_idx): Linear(in_features=1, out_features=256, bias=True)\n",
       "        (open): Linear(in_features=1, out_features=256, bias=True)\n",
       "        (high): Linear(in_features=1, out_features=256, bias=True)\n",
       "        (low): Linear(in_features=1, out_features=256, bias=True)\n",
       "        (target_1): Linear(in_features=1, out_features=256, bias=True)\n",
       "        (target_2): Linear(in_features=1, out_features=256, bias=True)\n",
       "        (vopen): Linear(in_features=1, out_features=256, bias=True)\n",
       "        (vhigh): Linear(in_features=1, out_features=256, bias=True)\n",
       "        (vlow): Linear(in_features=1, out_features=256, bias=True)\n",
       "        (VIX): Linear(in_features=1, out_features=256, bias=True)\n",
       "        (SPY): Linear(in_features=1, out_features=256, bias=True)\n",
       "        (TNX): Linear(in_features=1, out_features=256, bias=True)\n",
       "        (rsi14): Linear(in_features=1, out_features=256, bias=True)\n",
       "        (rsi9): Linear(in_features=1, out_features=256, bias=True)\n",
       "        (rsi24): Linear(in_features=1, out_features=256, bias=True)\n",
       "        (MACD5355macddiff): Linear(in_features=1, out_features=256, bias=True)\n",
       "        (MACD5355macddiffslope): Linear(in_features=1, out_features=256, bias=True)\n",
       "        (MACD5355macd): Linear(in_features=1, out_features=256, bias=True)\n",
       "        (MACD5355macdslope): Linear(in_features=1, out_features=256, bias=True)\n",
       "        (MACD5355macdsig): Linear(in_features=1, out_features=256, bias=True)\n",
       "        (MACD5355macdsigslope): Linear(in_features=1, out_features=256, bias=True)\n",
       "        (MACD12269macddiff): Linear(in_features=1, out_features=256, bias=True)\n",
       "        (MACD12269macddiffslope): Linear(in_features=1, out_features=256, bias=True)\n",
       "        (MACD12269macd): Linear(in_features=1, out_features=256, bias=True)\n",
       "        (MACD12269macdslope): Linear(in_features=1, out_features=256, bias=True)\n",
       "        (MACD12269macdsig): Linear(in_features=1, out_features=256, bias=True)\n",
       "        (MACD12269macdsigslope): Linear(in_features=1, out_features=256, bias=True)\n",
       "        (lowTail): Linear(in_features=1, out_features=256, bias=True)\n",
       "        (highTail): Linear(in_features=1, out_features=256, bias=True)\n",
       "        (openTail): Linear(in_features=1, out_features=256, bias=True)\n",
       "        (IntradayBar): Linear(in_features=1, out_features=256, bias=True)\n",
       "        (IntradayRange): Linear(in_features=1, out_features=256, bias=True)\n",
       "        (CloseOverSMA5): Linear(in_features=1, out_features=256, bias=True)\n",
       "        (CloseOverSMA10): Linear(in_features=1, out_features=256, bias=True)\n",
       "        (CloseOverSMA12): Linear(in_features=1, out_features=256, bias=True)\n",
       "        (CloseOverSMA20): Linear(in_features=1, out_features=256, bias=True)\n",
       "        (CloseOverSMA30): Linear(in_features=1, out_features=256, bias=True)\n",
       "        (CloseOverSMA65): Linear(in_features=1, out_features=256, bias=True)\n",
       "        (CloseOverSMA50): Linear(in_features=1, out_features=256, bias=True)\n",
       "        (CloseOverSMA100): Linear(in_features=1, out_features=256, bias=True)\n",
       "        (CloseOverSMA200): Linear(in_features=1, out_features=256, bias=True)\n",
       "        (VolOverSMA5): Linear(in_features=1, out_features=256, bias=True)\n",
       "        (VolOverSMA10): Linear(in_features=1, out_features=256, bias=True)\n",
       "        (VolOverSMA12): Linear(in_features=1, out_features=256, bias=True)\n",
       "        (VolOverSMA20): Linear(in_features=1, out_features=256, bias=True)\n",
       "        (VolOverSMA30): Linear(in_features=1, out_features=256, bias=True)\n",
       "        (VolOverSMA65): Linear(in_features=1, out_features=256, bias=True)\n",
       "        (VolOverSMA50): Linear(in_features=1, out_features=256, bias=True)\n",
       "        (VolOverSMA100): Linear(in_features=1, out_features=256, bias=True)\n",
       "        (VolOverSMA200): Linear(in_features=1, out_features=256, bias=True)\n",
       "        (Ret1day): Linear(in_features=1, out_features=256, bias=True)\n",
       "        (Ret4day): Linear(in_features=1, out_features=256, bias=True)\n",
       "        (Ret8day): Linear(in_features=1, out_features=256, bias=True)\n",
       "        (Ret12day): Linear(in_features=1, out_features=256, bias=True)\n",
       "        (Ret24day): Linear(in_features=1, out_features=256, bias=True)\n",
       "        (Ret72day): Linear(in_features=1, out_features=256, bias=True)\n",
       "        (Ret240day): Linear(in_features=1, out_features=256, bias=True)\n",
       "        (RSC): Linear(in_features=1, out_features=256, bias=True)\n",
       "        (bands_l): Linear(in_features=1, out_features=256, bias=True)\n",
       "        (bands_u): Linear(in_features=1, out_features=256, bias=True)\n",
       "        (ADX): Linear(in_features=1, out_features=256, bias=True)\n",
       "        (cloudA): Linear(in_features=1, out_features=256, bias=True)\n",
       "        (cloudB): Linear(in_features=1, out_features=256, bias=True)\n",
       "        (closeVsIchA): Linear(in_features=1, out_features=256, bias=True)\n",
       "        (closeVsIchB): Linear(in_features=1, out_features=256, bias=True)\n",
       "        (IchAvIchB): Linear(in_features=1, out_features=256, bias=True)\n",
       "        (CondVol_1): Linear(in_features=1, out_features=256, bias=True)\n",
       "        (CondVol_4): Linear(in_features=1, out_features=256, bias=True)\n",
       "        (CondVol_8): Linear(in_features=1, out_features=256, bias=True)\n",
       "        (CondVol_12): Linear(in_features=1, out_features=256, bias=True)\n",
       "        (CondVol_24): Linear(in_features=1, out_features=256, bias=True)\n",
       "        (CondVol_72): Linear(in_features=1, out_features=256, bias=True)\n",
       "        (CondVol_240): Linear(in_features=1, out_features=256, bias=True)\n",
       "        (CV1vCV4): Linear(in_features=1, out_features=256, bias=True)\n",
       "        (CV4vCV8): Linear(in_features=1, out_features=256, bias=True)\n",
       "        (CV8vCV12): Linear(in_features=1, out_features=256, bias=True)\n",
       "        (CV12vCV24): Linear(in_features=1, out_features=256, bias=True)\n",
       "        (CV8vCV24): Linear(in_features=1, out_features=256, bias=True)\n",
       "        (CV24vCV240): Linear(in_features=1, out_features=256, bias=True)\n",
       "        (RSC_VIX): Linear(in_features=1, out_features=256, bias=True)\n",
       "        (RSC_VIX_IV): Linear(in_features=1, out_features=256, bias=True)\n",
       "        (RSC_VIX_real): Linear(in_features=1, out_features=256, bias=True)\n",
       "        (RSC_VIX_IV_real): Linear(in_features=1, out_features=256, bias=True)\n",
       "        (RSC_IV_gar): Linear(in_features=1, out_features=256, bias=True)\n",
       "        (close_spy_corr22): Linear(in_features=1, out_features=256, bias=True)\n",
       "        (close_tnx_corr22): Linear(in_features=1, out_features=256, bias=True)\n",
       "        (vclose_VIX_corr22): Linear(in_features=1, out_features=256, bias=True)\n",
       "        (garch_IV_corr22): Linear(in_features=1, out_features=256, bias=True)\n",
       "        (close_spy_corr65): Linear(in_features=1, out_features=256, bias=True)\n",
       "        (close_tnx_corr65): Linear(in_features=1, out_features=256, bias=True)\n",
       "        (vclose_VIX_corr65): Linear(in_features=1, out_features=256, bias=True)\n",
       "        (garch_IV_corr65): Linear(in_features=1, out_features=256, bias=True)\n",
       "        (close_spy_corr252): Linear(in_features=1, out_features=256, bias=True)\n",
       "        (close_tnx_corr252): Linear(in_features=1, out_features=256, bias=True)\n",
       "        (vclose_VIX_corr252): Linear(in_features=1, out_features=256, bias=True)\n",
       "        (garch_IV_corr252): Linear(in_features=1, out_features=256, bias=True)\n",
       "      )\n",
       "      (softmax): Softmax(dim=-1)\n",
       "    )\n",
       "    (decoder_variable_selection): VariableSelectionNetwork(\n",
       "      (single_variable_grns): ModuleDict(\n",
       "        (time_idx): GatedResidualNetwork(\n",
       "          (fc1): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (elu): ELU(alpha=1.0)\n",
       "          (fc2): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (gate_norm): GateAddNorm(\n",
       "            (glu): GatedLinearUnit(\n",
       "              (dropout): Dropout(p=0.35, inplace=False)\n",
       "              (fc): Linear(in_features=256, out_features=512, bias=True)\n",
       "            )\n",
       "            (add_norm): AddNorm(\n",
       "              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (prescalers): ModuleDict(\n",
       "        (time_idx): Linear(in_features=1, out_features=256, bias=True)\n",
       "      )\n",
       "      (softmax): Softmax(dim=-1)\n",
       "    )\n",
       "    (static_context_variable_selection): GatedResidualNetwork(\n",
       "      (fc1): Linear(in_features=256, out_features=256, bias=True)\n",
       "      (elu): ELU(alpha=1.0)\n",
       "      (fc2): Linear(in_features=256, out_features=256, bias=True)\n",
       "      (gate_norm): GateAddNorm(\n",
       "        (glu): GatedLinearUnit(\n",
       "          (dropout): Dropout(p=0.35, inplace=False)\n",
       "          (fc): Linear(in_features=256, out_features=512, bias=True)\n",
       "        )\n",
       "        (add_norm): AddNorm(\n",
       "          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (static_context_initial_hidden_lstm): GatedResidualNetwork(\n",
       "      (fc1): Linear(in_features=256, out_features=256, bias=True)\n",
       "      (elu): ELU(alpha=1.0)\n",
       "      (fc2): Linear(in_features=256, out_features=256, bias=True)\n",
       "      (gate_norm): GateAddNorm(\n",
       "        (glu): GatedLinearUnit(\n",
       "          (dropout): Dropout(p=0.35, inplace=False)\n",
       "          (fc): Linear(in_features=256, out_features=512, bias=True)\n",
       "        )\n",
       "        (add_norm): AddNorm(\n",
       "          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (static_context_initial_cell_lstm): GatedResidualNetwork(\n",
       "      (fc1): Linear(in_features=256, out_features=256, bias=True)\n",
       "      (elu): ELU(alpha=1.0)\n",
       "      (fc2): Linear(in_features=256, out_features=256, bias=True)\n",
       "      (gate_norm): GateAddNorm(\n",
       "        (glu): GatedLinearUnit(\n",
       "          (dropout): Dropout(p=0.35, inplace=False)\n",
       "          (fc): Linear(in_features=256, out_features=512, bias=True)\n",
       "        )\n",
       "        (add_norm): AddNorm(\n",
       "          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (static_context_enrichment): GatedResidualNetwork(\n",
       "      (fc1): Linear(in_features=256, out_features=256, bias=True)\n",
       "      (elu): ELU(alpha=1.0)\n",
       "      (fc2): Linear(in_features=256, out_features=256, bias=True)\n",
       "      (gate_norm): GateAddNorm(\n",
       "        (glu): GatedLinearUnit(\n",
       "          (dropout): Dropout(p=0.35, inplace=False)\n",
       "          (fc): Linear(in_features=256, out_features=512, bias=True)\n",
       "        )\n",
       "        (add_norm): AddNorm(\n",
       "          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (lstm_encoder): LSTM(256, 256, num_layers=2, batch_first=True, dropout=0.35)\n",
       "    (lstm_decoder): LSTM(256, 256, num_layers=2, batch_first=True, dropout=0.35)\n",
       "    (post_lstm_gate_encoder): GatedLinearUnit(\n",
       "      (dropout): Dropout(p=0.35, inplace=False)\n",
       "      (fc): Linear(in_features=256, out_features=512, bias=True)\n",
       "    )\n",
       "    (post_lstm_gate_decoder): GatedLinearUnit(\n",
       "      (dropout): Dropout(p=0.35, inplace=False)\n",
       "      (fc): Linear(in_features=256, out_features=512, bias=True)\n",
       "    )\n",
       "    (post_lstm_add_norm_encoder): AddNorm(\n",
       "      (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (post_lstm_add_norm_decoder): AddNorm(\n",
       "      (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (static_enrichment): GatedResidualNetwork(\n",
       "      (fc1): Linear(in_features=256, out_features=256, bias=True)\n",
       "      (elu): ELU(alpha=1.0)\n",
       "      (context): Linear(in_features=256, out_features=256, bias=False)\n",
       "      (fc2): Linear(in_features=256, out_features=256, bias=True)\n",
       "      (gate_norm): GateAddNorm(\n",
       "        (glu): GatedLinearUnit(\n",
       "          (dropout): Dropout(p=0.35, inplace=False)\n",
       "          (fc): Linear(in_features=256, out_features=512, bias=True)\n",
       "        )\n",
       "        (add_norm): AddNorm(\n",
       "          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (multihead_attn): InterpretableMultiHeadAttention(\n",
       "      (dropout): Dropout(p=0.35, inplace=False)\n",
       "      (v_layer): Linear(in_features=256, out_features=64, bias=True)\n",
       "      (q_layers): ModuleList(\n",
       "        (0-3): 4 x Linear(in_features=256, out_features=64, bias=True)\n",
       "      )\n",
       "      (k_layers): ModuleList(\n",
       "        (0-3): 4 x Linear(in_features=256, out_features=64, bias=True)\n",
       "      )\n",
       "      (attention): ScaledDotProductAttention(\n",
       "        (softmax): Softmax(dim=2)\n",
       "      )\n",
       "      (w_h): Linear(in_features=64, out_features=256, bias=False)\n",
       "    )\n",
       "    (post_attn_gate_norm): GateAddNorm(\n",
       "      (glu): GatedLinearUnit(\n",
       "        (dropout): Dropout(p=0.35, inplace=False)\n",
       "        (fc): Linear(in_features=256, out_features=512, bias=True)\n",
       "      )\n",
       "      (add_norm): AddNorm(\n",
       "        (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (pos_wise_ff): GatedResidualNetwork(\n",
       "      (fc1): Linear(in_features=256, out_features=256, bias=True)\n",
       "      (elu): ELU(alpha=1.0)\n",
       "      (fc2): Linear(in_features=256, out_features=256, bias=True)\n",
       "      (gate_norm): GateAddNorm(\n",
       "        (glu): GatedLinearUnit(\n",
       "          (dropout): Dropout(p=0.35, inplace=False)\n",
       "          (fc): Linear(in_features=256, out_features=512, bias=True)\n",
       "        )\n",
       "        (add_norm): AddNorm(\n",
       "          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pre_output_gate_norm): GateAddNorm(\n",
       "      (glu): GatedLinearUnit(\n",
       "        (fc): Linear(in_features=256, out_features=512, bias=True)\n",
       "      )\n",
       "      (add_norm): AddNorm(\n",
       "        (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (output_layer): ModuleList(\n",
       "      (0-1): 2 x Linear(in_features=256, out_features=1, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from pytorch_forecasting.data import TimeSeriesDataSet\n",
    "from pytorch_forecasting.models import TemporalFusionTransformer\n",
    "from pytorch_forecasting.metrics import MultiLoss, QuantileLoss, MAE\n",
    "from pytorch_forecasting.data.encoders import MultiNormalizer, TorchNormalizer\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import LightningModule, Trainer\n",
    "from pytorch_lightning.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from pytorch_lightning.loggers import CSVLogger\n",
    "### Model Path\n",
    "modelPath=\"mar3_model.pth\"\n",
    "\n",
    "###\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch.set_float32_matmul_precision(\"medium\")  # For NVIDIA Tensor Cores\n",
    "print(\"Using device:\", DEVICE)\n",
    "\n",
    "# -----------------------------\n",
    "# 2. Data Loading\n",
    "# -----------------------------\n",
    "def load_data(folder):\n",
    "    all_files = [os.path.join(folder, f) for f in os.listdir(folder) if f.endswith('.csv')]\n",
    "    dfs = []\n",
    "    for file in all_files:\n",
    "        # df = pd.read_csv(file, index_col=0)  # When There is no Date index\n",
    "        df = pd.read_csv(file)\n",
    "        df=df.reset_index(drop=True)\n",
    "        df=df.drop('date',axis=1) # Drops 'date' column\n",
    "        df[\"time_idx\"] = range(len(df))\n",
    "        df[\"group\"] = os.path.basename(file).split('.')[0]\n",
    "        df[\"group\"] = df[\"group\"].astype(str)\n",
    "        df.rename(columns={\"Close\": \"target_1\", \"vclose\": \"target_2\"}, inplace=True)\n",
    "        dfs.append(df)\n",
    "    return pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "train_data_folder = \"data/train\"\n",
    "test_data_folder  = \"data/test\"\n",
    "oos_data_folder   = \"data/oos\"\n",
    "\n",
    "train_df = load_data(train_data_folder)\n",
    "test_df  = load_data(test_data_folder)\n",
    "oos_df   = load_data(oos_data_folder)\n",
    "\n",
    "# -----------------------------\n",
    "# 3. Multi-Target Dataset\n",
    "# -----------------------------\n",
    "training = TimeSeriesDataSet(\n",
    "    train_df,\n",
    "    time_idx=\"time_idx\",\n",
    "    target=[\"target_1\", \"target_2\"],\n",
    "    group_ids=[\"group\"],\n",
    "    max_encoder_length=90,\n",
    "    max_prediction_length=5,\n",
    "    static_categoricals=[\"group\"],\n",
    "    time_varying_known_reals=[\"time_idx\"],\n",
    "    time_varying_unknown_reals=[\n",
    "        c for c in train_df.columns if c not in [\"group\", \"time_idx\"]#, \"target_1\", \"target_2\"]\n",
    "    ],\n",
    "    target_normalizer=MultiNormalizer([\n",
    "        TorchNormalizer(method=\"identity\"),\n",
    "        TorchNormalizer(method=\"identity\")\n",
    "    ])\n",
    ")\n",
    "\n",
    "# validation & OOS sets with predict_mode=False -> keep target data\n",
    "validation = TimeSeriesDataSet.from_dataset(training, test_df, predict_mode=False)\n",
    "oos        = TimeSeriesDataSet.from_dataset(training, oos_df,  predict_mode=False)\n",
    "\n",
    "# -----------------------------\n",
    "# 4. DataLoaders\n",
    "# -----------------------------\n",
    "train_dataloader = training.to_dataloader(\n",
    "    train=True, batch_size=64, shuffle=True, num_workers=16, pin_memory=False\n",
    ")\n",
    "val_dataloader = validation.to_dataloader(\n",
    "    train=False, batch_size=64, shuffle=False, num_workers=16, pin_memory=False\n",
    ")\n",
    "oos_dataloader = oos.to_dataloader(\n",
    "    train=False, batch_size=16, shuffle=False, num_workers=16, pin_memory=False\n",
    ")\n",
    "\n",
    "# -----------------------------\n",
    "# 5. Define Multi-Target TFT Model w/ Quantile Loss\n",
    "# -----------------------------\n",
    "import torch.nn as nn\n",
    "\n",
    "tft = TemporalFusionTransformer.from_dataset(\n",
    "    training,\n",
    "    learning_rate=4.171863120490385e-05,\n",
    "    lstm_layers=2,\n",
    "    hidden_size=256,\n",
    "    attention_head_size=4,\n",
    "    dropout=0.35,\n",
    "    hidden_continuous_size=256,\n",
    "    output_size=[1, 1],  # single output per target for MSE\n",
    "    loss=MultiLoss([\n",
    "        nn.MSELoss(),\n",
    "        nn.MSELoss()\n",
    "    ]),\n",
    "    log_interval=200,\n",
    "    reduce_on_plateau_patience=5,\n",
    ").to(DEVICE)\n",
    "\n",
    "print(f\"Number of params in network: {tft.size() / 1e3:.1f}k\")\n",
    "\n",
    "# -----------------------------\n",
    "# 6. LightningModule\n",
    "# -----------------------------\n",
    "class TFTLightningModule(LightningModule):\n",
    "    def __init__(self, tft_model):\n",
    "        super().__init__()\n",
    "        self.tft_model = tft_model\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.tft_model(x)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        # 'y' is a tuple: ([target0_tensor, target1_tensor], None)\n",
    "        out = self(x)\n",
    "        pred = out[\"prediction\"]  # list: 0 => target0, 1 => target1\n",
    "        loss = self.tft_model.loss(pred, y)  # automatically handles multi-target\n",
    "        self.log(\"train_loss\", loss, prog_bar=True, on_epoch=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        out = self(x)\n",
    "        pred = out[\"prediction\"]\n",
    "        loss = self.tft_model.loss(pred, y)\n",
    "        self.log(\"val_loss\", loss, prog_bar=True, on_epoch=True)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return self.tft_model.configure_optimizers()\n",
    "\n",
    "    def predict_step(self, batch, batch_idx, dataloader_idx=0):\n",
    "        # If the batch is a (x, y) tuple, we only pass x to the model\n",
    "        if isinstance(batch, (tuple, list)):\n",
    "            x = batch[0]\n",
    "        else:\n",
    "            x = batch\n",
    "        return self(x)\n",
    "\n",
    "tft_module = TFTLightningModule(tft).to(DEVICE)\n",
    "\n",
    "# Optional: training setup\n",
    "early_stop_callback = EarlyStopping(monitor=\"val_loss\", patience=7, mode=\"min\")\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    dirpath=\"checkpoints/\",\n",
    "    filename=\"my-tft-{epoch:02d}-{val_loss:.2f}\",\n",
    "    save_top_k=1,\n",
    "    monitor=\"val_loss\",\n",
    "    mode=\"min\"\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    max_epochs=35,   # set higher for real training\n",
    "    accelerator=\"gpu\" if DEVICE == \"cuda\" else \"cpu\",\n",
    "    devices=1,\n",
    "    precision=32,\n",
    "    logger=CSVLogger(\"logs\", name=\"tft_multi_target_quantile\"),\n",
    "    callbacks=[early_stop_callback, checkpoint_callback],\n",
    ")\n",
    "\n",
    "# --- Load the saved state dict ---\n",
    "state_dict = torch.load(modelPath, map_location=DEVICE)\n",
    "tft_module.load_state_dict(state_dict)\n",
    "tft_module.to(DEVICE)\n",
    "tft_module.eval()  # set the model to evaluation mode\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict w/ loaded model. RSM ranking from validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing CSCO ...\n",
      "Processing ISRG ...\n",
      "Processing BA ...\n",
      "Processing VRTX ...\n",
      "Processing GILD ...\n",
      "Processing EQIX ...\n",
      "Processing MDT ...\n",
      "Processing V ...\n",
      "Processing MO ...\n",
      "Processing CDNS ...\n",
      "Processing HCA ...\n",
      "Processing AJG ...\n",
      "Processing C ...\n",
      "Processing T ...\n",
      "Processing APH ...\n",
      "Processing MSI ...\n",
      "Processing FCX ...\n",
      "Processing BAC ...\n",
      "Processing PSX ...\n",
      "Processing ADI ...\n",
      "Processing ADBE ...\n",
      "Processing CPRT ...\n",
      "Processing TDG ...\n",
      "Processing SYK ...\n",
      "Processing CB ...\n",
      "Processing NOW ...\n",
      "Processing LLY ...\n",
      "Processing COST ...\n",
      "Processing LOW ...\n",
      "Processing MDLZ ...\n",
      "Processing BKNG ...\n",
      "Processing MET ...\n",
      "Processing DLR ...\n",
      "Processing TJX ...\n",
      "Processing MPC ...\n",
      "Processing D ...\n",
      "Processing MRK ...\n",
      "Processing NOC ...\n",
      "Processing UNP ...\n",
      "Processing ABBV ...\n",
      "Processing ORCL ...\n",
      "Processing ECL ...\n",
      "Processing SBUX ...\n",
      "Processing AMT ...\n",
      "Processing INTU ...\n",
      "Processing PG ...\n",
      "Processing CAT ...\n",
      "Processing MCD ...\n",
      "Processing AMZN ...\n",
      "Processing INTC ...\n",
      "Processing BDX ...\n",
      "Processing KMI ...\n",
      "Processing WELL ...\n",
      "Processing GM ...\n",
      "Processing TXN ...\n",
      "Processing FI ...\n",
      "Processing TMO ...\n",
      "Processing MMM ...\n",
      "Processing FTNT ...\n",
      "Processing ADSK ...\n",
      "Processing KO ...\n",
      "Processing PCAR ...\n",
      "Processing NEE ...\n",
      "Processing UPS ...\n",
      "Processing ELV ...\n",
      "Processing EMR ...\n",
      "Processing MSFT ...\n",
      "Processing CTAS ...\n",
      "Processing ACN ...\n",
      "Processing CMG ...\n",
      "Processing SHW ...\n",
      "Processing AMAT ...\n",
      "Processing DE ...\n",
      "Processing SPG ...\n",
      "Processing KLAC ...\n",
      "Processing RTX ...\n",
      "Processing NXPI ...\n",
      "Processing PNC ...\n",
      "Processing NVDA ...\n",
      "Processing ROP ...\n",
      "Processing HD ...\n",
      "Processing AON ...\n",
      "Processing ZTS ...\n",
      "Processing FDX ...\n",
      "Processing SCHW ...\n",
      "Processing AZO ...\n",
      "Processing AXP ...\n",
      "Processing DFS ...\n",
      "Processing SO ...\n",
      "Processing CME ...\n",
      "Processing XOM ...\n",
      "Processing AMP ...\n",
      "Processing CVX ...\n",
      "Processing CMCSA ...\n",
      "Processing ICE ...\n",
      "Processing NSC ...\n",
      "Processing NKE ...\n",
      "Processing CMI ...\n",
      "Processing PLD ...\n",
      "Processing IBM ...\n",
      "Processing USB ...\n",
      "Processing BSX ...\n",
      "Processing ITW ...\n",
      "Processing EOG ...\n",
      "Processing KMB ...\n",
      "Processing SPGI ...\n",
      "Processing NEM ...\n",
      "Processing WFC ...\n",
      "Processing GS ...\n",
      "Processing GD ...\n",
      "Processing PM ...\n",
      "Processing MCO ...\n",
      "Processing PANW ...\n",
      "Processing DIS ...\n",
      "Processing GE ...\n",
      "Processing ALL ...\n",
      "Processing ETN ...\n",
      "Processing NFLX ...\n",
      "Processing CVS ...\n",
      "Processing JPM ...\n",
      "Processing ABT ...\n",
      "Processing COF ...\n",
      "Processing PH ...\n",
      "Processing TSLA ...\n",
      "Processing COP ...\n",
      "Processing DHR ...\n",
      "Processing MCK ...\n",
      "Processing GOOGL ...\n",
      "Processing PAYX ...\n",
      "Processing META ...\n",
      "Processing MMC ...\n",
      "Processing SRE ...\n",
      "Processing ORLY ...\n",
      "Processing RCL ...\n",
      "Processing SNPS ...\n",
      "Processing PFE ...\n",
      "Processing DUK ...\n",
      "Processing REGN ...\n",
      "Processing CL ...\n",
      "Processing VZ ...\n",
      "Processing JCI ...\n",
      "Processing AMGN ...\n",
      "Processing ADP ...\n",
      "Processing RSG ...\n",
      "Processing QCOM ...\n",
      "Processing MS ...\n",
      "Processing OKE ...\n",
      "Processing BK ...\n",
      "Processing HWM ...\n",
      "Processing TFC ...\n",
      "Processing AFL ...\n",
      "Processing BRK B ...\n",
      "Processing UNH ...\n",
      "Processing WMB ...\n",
      "Processing AIG ...\n",
      "Processing MA ...\n",
      "Processing HON ...\n",
      "Processing O ...\n",
      "Processing SLB ...\n",
      "Processing TT ...\n",
      "Processing TGT ...\n",
      "Processing AAPL ...\n",
      "Processing APD ...\n",
      "Processing BX ...\n",
      "Processing WMT ...\n",
      "Processing LMT ...\n",
      "Processing KKR ...\n",
      "Processing BMY ...\n",
      "Processing PSA ...\n",
      "Processing MU ...\n",
      "Processing TRV ...\n",
      "Processing AEP ...\n",
      "Processing CI ...\n",
      "Processing JNJ ...\n",
      "Processing WM ...\n",
      "Processing CRM ...\n",
      "Processing PGR ...\n",
      "Processing LRCX ...\n",
      "Processing BLK ...\n",
      "\n",
      "Results DataFrame:\n",
      "    Symbol       MSE     Pred1     Pred2     Pred3     Pred4     Pred5  \\\n",
      "0     CSCO  0.073799  2.228963  2.205342  2.197472  2.198900  2.202812   \n",
      "1     ISRG  0.004205  1.884429  1.887331  1.899634  1.919682  1.941538   \n",
      "2       BA  0.140395  0.551384  0.559629  0.568269  0.589104  0.611944   \n",
      "3     VRTX  0.066153  0.908752  0.885531  0.856358  0.847197  0.858112   \n",
      "4     GILD  0.234858  2.599140  2.504758  2.451344  2.419095  2.401365   \n",
      "..     ...       ...       ...       ...       ...       ...       ...   \n",
      "174     WM  0.058512  2.498276  2.461812  2.417433  2.390239  2.376925   \n",
      "175    CRM  0.008703  0.726581  0.687723  0.663731  0.679475  0.721078   \n",
      "176    PGR  0.026139  1.873076  1.872382  1.873746  1.896719  1.930308   \n",
      "177   LRCX  0.021431  0.071776  0.071463  0.080527  0.111621  0.150098   \n",
      "178    BLK  0.430638  1.045340  1.052952  1.053889  1.066353  1.084257   \n",
      "\n",
      "        Delta  \n",
      "0   -0.026151  \n",
      "1    0.057109  \n",
      "2    0.060560  \n",
      "3   -0.050641  \n",
      "4   -0.197775  \n",
      "..        ...  \n",
      "174 -0.121351  \n",
      "175 -0.005503  \n",
      "176  0.057232  \n",
      "177  0.078322  \n",
      "178  0.038917  \n",
      "\n",
      "[179 rows x 8 columns]\n",
      "\n",
      "Ranked by MSE:\n",
      "    Symbol       MSE     Pred1     Pred2     Pred3     Pred4     Pred5  \\\n",
      "114     GE  0.002473  1.857839  1.842645  1.843751  1.864074  1.888420   \n",
      "1     ISRG  0.004205  1.884429  1.887331  1.899634  1.919682  1.941538   \n",
      "126    MCK  0.005388  1.146410  1.128212  1.112221  1.123402  1.161281   \n",
      "148    HWM  0.005714  1.874273  1.832578  1.819144  1.832610  1.859268   \n",
      "112   PANW  0.006932  1.383082  1.407922  1.427431  1.447760  1.467361   \n",
      "..     ...       ...       ...       ...       ...       ...       ...   \n",
      "162    APD  0.927897  1.119471  1.111449  1.110071  1.124122  1.148553   \n",
      "150    AFL  0.949289  0.618173  0.579612  0.551568  0.565603  0.626799   \n",
      "102    ITW  1.127481  1.250133  1.264702  1.282557  1.332197  1.395409   \n",
      "123   TSLA  1.241562  1.146391  1.121180  1.105012  1.108525  1.123252   \n",
      "152    UNH  2.435796 -1.162530 -1.146399 -1.106329 -1.066894 -1.049152   \n",
      "\n",
      "        Delta  \n",
      "114  0.030581  \n",
      "1    0.057109  \n",
      "126  0.014870  \n",
      "148 -0.015005  \n",
      "112  0.084279  \n",
      "..        ...  \n",
      "162  0.029083  \n",
      "150  0.008625  \n",
      "102  0.145277  \n",
      "123 -0.023139  \n",
      "152  0.113378  \n",
      "\n",
      "[179 rows x 8 columns]\n"
     ]
    }
   ],
   "source": [
    "saveAs = 'RankedPreds_3-02_newModel.csv'\n",
    "\n",
    "mu_sig_df = pd.read_csv('./data/TixMuSig.csv')\n",
    "\n",
    "def move_to_device(batch_x, device):\n",
    "    \"\"\"Ensure all Tensors in batch_x are on the same device.\"\"\"\n",
    "    if isinstance(batch_x, torch.Tensor):\n",
    "        return batch_x.to(device)\n",
    "    elif isinstance(batch_x, dict):\n",
    "        return {k: move_to_device(v, device) for k, v in batch_x.items()}\n",
    "    elif isinstance(batch_x, list):\n",
    "        return [move_to_device(item, device) for item in batch_x]\n",
    "    else:\n",
    "        return batch_x\n",
    "\n",
    "def process_symbol(symbol):\n",
    "    # Filter the full oos dataframe for this symbol (group)\n",
    "    stock_oos_df = oos_df[oos_df[\"group\"] == symbol]\n",
    "    if stock_oos_df.empty:\n",
    "        print(f\"No OOS data for {symbol}.\")\n",
    "        return None\n",
    "\n",
    "    # Retrieve the scaling parameters for price (target_1) from mu_sig_df.\n",
    "    try:\n",
    "        mu_p = mu_sig_df.loc[mu_sig_df['ticker'] == symbol, 'closemu'].values[0]\n",
    "        sig_p = mu_sig_df.loc[mu_sig_df['ticker'] == symbol, 'closesig'].values[0]\n",
    "    except Exception as e:\n",
    "        print(f\"Error retrieving mu/sig for {symbol}: {e}\")\n",
    "        return None\n",
    "\n",
    "    # -------------------------------------------------------------------\n",
    "    # Create two datasets:\n",
    "    # 1. For current predictions (no future targets provided)\n",
    "    eq_dataset_current = TimeSeriesDataSet.from_dataset(\n",
    "        training, stock_oos_df, predict_mode=True\n",
    "    )\n",
    "    eq_dataloader_current = eq_dataset_current.to_dataloader(\n",
    "        train=False,\n",
    "        batch_size=len(eq_dataset_current),  # All samples in one batch\n",
    "        shuffle=False,\n",
    "        num_workers=0,\n",
    "        pin_memory=False\n",
    "    )\n",
    "\n",
    "    # 2. For backtesting (to compute MSE, future values are provided)\n",
    "    eq_dataset_backtest = TimeSeriesDataSet.from_dataset(\n",
    "        training, stock_oos_df, predict_mode=False\n",
    "    )\n",
    "    eq_dataloader_backtest = eq_dataset_backtest.to_dataloader(\n",
    "        train=False,\n",
    "        batch_size=len(eq_dataset_backtest),  # All samples in one batch\n",
    "        shuffle=False,\n",
    "        num_workers=0,\n",
    "        pin_memory=False\n",
    "    )\n",
    "    # -------------------------------------------------------------------\n",
    "\n",
    "    # --- Get current predictions (using predict_mode=True) ---\n",
    "    with torch.no_grad():\n",
    "        for batch in eq_dataloader_current:\n",
    "            x_current, _ = batch  # y is not used in predict_mode=True\n",
    "            x_current = move_to_device(x_current, DEVICE)\n",
    "            out_current = tft_module(x_current)\n",
    "            preds_current = out_current[\"prediction\"]\n",
    "            # Assuming target_1 is at index 0 and we use the median (index 0 when using MSE)\n",
    "            current_price_preds = preds_current[0][:, :, 0].cpu()  # shape: (batch, prediction_length)\n",
    "\n",
    "    # Extract Pred1–Pred5 from current predictions:\n",
    "    # (Assuming prediction_length is 5 and batch size is 1)\n",
    "    pred1 = current_price_preds[0, 0].item()\n",
    "    pred2 = current_price_preds[0, 1].item()\n",
    "    pred3 = current_price_preds[0, 2].item()\n",
    "    pred4 = current_price_preds[0, 3].item()\n",
    "    pred5 = current_price_preds[0, 4].item()\n",
    "    # print(current_price_preds[0])\n",
    "\n",
    "\n",
    "    # --- Get backtest predictions for MSE calculation (using predict_mode=False) ---\n",
    "    with torch.no_grad():\n",
    "        for batch in eq_dataloader_backtest:\n",
    "            x_backtest, y_tuple = batch\n",
    "            y_list, _ = y_tuple\n",
    "            # Get the actual future values for target_1.\n",
    "            actual_future = y_list[0]  # shape: [batch, prediction_length]\n",
    "            x_backtest = move_to_device(x_backtest, DEVICE)\n",
    "            out_backtest = tft_module(x_backtest)\n",
    "            preds_backtest = out_backtest[\"prediction\"]\n",
    "            backtest_price_preds = preds_backtest[0][:, :, 0].cpu()\n",
    "\n",
    "    # Compute MSE over the forecast horizon using the backtest data.\n",
    "    mse_value = torch.mean((backtest_price_preds[0] - actual_future[0])**2).item()\n",
    "    # plt.plot(backtest_price_preds)\n",
    "    # plt.plot(actual_future)\n",
    "    # plt.show()\n",
    "    # Build the result dictionary.\n",
    "    result = {\n",
    "        \"Symbol\": symbol,\n",
    "        \"MSE\": mse_value,\n",
    "        \"Pred1\": pred1,\n",
    "        \"Pred2\": pred2,\n",
    "        \"Pred3\": pred3,\n",
    "        \"Pred4\": pred4,\n",
    "        \"Pred5\": pred5,\n",
    "        \"Delta\": pred5-pred1,\n",
    "    }\n",
    "    return result\n",
    "\n",
    "# Loop over each CSV file in \"./data/oos\" and collect results.\n",
    "results_list = []\n",
    "oos_folder = \"./data/oos\"\n",
    "oos_files = [f for f in os.listdir(oos_folder) if f.endswith('.csv')]\n",
    "for file in oos_files:#[:6]:\n",
    "    symbol = os.path.splitext(file)[0].upper()\n",
    "    print(f\"Processing {symbol} ...\")\n",
    "    res = process_symbol(symbol)\n",
    "    if res is not None:\n",
    "        results_list.append(res)\n",
    "\n",
    "results_df = pd.DataFrame(results_list)\n",
    "print(\"\\nResults DataFrame:\")\n",
    "print(results_df)\n",
    "\n",
    "results_df_sorted = results_df.sort_values(by=\"MSE\")\n",
    "print(\"\\nRanked by MSE:\")\n",
    "print(results_df_sorted)\n",
    "results_df_sorted.to_csv(saveAs, index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
